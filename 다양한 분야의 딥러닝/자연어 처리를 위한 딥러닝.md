자연어 처리를 위한 딥러닝
04
04 자연어 처리를 위한 딥러닝
우리 주변의 예시 ‒ 자연어 처리
기계 번역 모델 음성 인식
자연어와 자연어 처리의 정의
자연어 처리(Natural Language Processing)
자연어의 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 일
04 자연어 처리를 위한 딥러닝
자연어 처리 Process
1. 자연어전처리
(Preprocessing)
2. 단어표현
(Word Embedding)
3. 모델적용하기
(Modeling)
04 자연어 처리를 위한 딥러닝
04 자연어 처리를 위한 딥러닝
자연어 전처리 방법
원 상태 그대로의 자연어는 전처리 과정이 필요함
Noise canceling
Tokenizing
StopWord removal
Noise Canceling
04 자연어 처리를 위한 딥러닝
“안녕하 세요. 반갑 스니다.” => “안녕하세요. 반갑습니다.”
자연어 문장의 스펠링 체크 및 띄어쓰기 오류 교정
Tokenizing
04 자연어 처리를 위한 딥러닝
“딥러닝 기초 과목을 수강하고 있습니다.”
=> ['딥', '러닝', '기초', '과목', '을', '수강', '하고', '있습니다', '.']
문장을 토큰(Token)으로 나눔,
토큰은 어절, 단어 등으로 목적에 따라 다르게 정의
StopWord removal
04 자연어 처리를 위한 딥러닝
한국어 stopword 예시)
아, 휴, 아이구, 아이쿠, 아이고, 쉿, 그렇지 않으면, 그러나, 그런데, 하지만, ...
불필요한 단어를 의미하는 불용어(StopWord) 제거
Confidential all right reserved
워드 임베딩(Word Embedding)
05
워드 임베딩(Word Embedding)의 정의
05
Embedding
워드 임베딩(Word Embedding)
Var1 Var2 Var3 Var4 Var5
Doc1
Doc2
Doc3
Doc4
Doc5
비정형 데이터를 정형데이터로 바꾸는 작업
워드 임베딩의 종류
05 워드 임베딩(Word Embedding)
Bag of Words
One-hot encoding
Document term matrix
Count-based Representations Distributed Representations
Word2vec
05 워드 임베딩(Word Embedding)
Bag of Words
자연어 데이터에 속해있는 단어들의 가방
자연어 데이터
[‘안녕’, ‘만나서’, ‘반가워’]
[‘안녕’, ‘나도’, ‘반가워’]
Bag of Words
[‘안녕’, ‘만나서’,
‘반가워’, ‘나도’]
05 워드 임베딩(Word Embedding)
One-hot encoding
학습 데이터의 모든 토큰을 크기로 한 벡터에서
해당 문장에 토큰이 존재하는지 확인
Bag of words
[‘안녕’, ‘만나서’,
‘반가워’, ‘나도’]
One-hot vector
[1, 1, 1, 0]
[1, 0, 1, 1]
05 워드 임베딩(Word Embedding)
Document term matrix
One hot encoding 결과에 빈도수 정보를 추가
정부가 발표하는 물가상승률과 소비자가
느끼는 물가상승률은 다르다.
('정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5,
'소비자': 6, '느끼는': 7, '은': 8, '다르다': 9)
[1, 2, 1, 1, 2, 1, 1, 1, 1, 1] 
05 워드 임베딩(Word Embedding)
Word2vec
단어들을 의미상으로 유사한 단어가
벡터공간에 가까이 있도록 Mapping 시키는 작업을 의미
특정 함수를 통해 우리가 원하는 차원으로 단어의 벡터를 Embedding 함
05 워드 임베딩(Word Embedding)
Word2vec 예시 텍스트
내가 어떻게 해야 그대를 잊을 수 있을까
‘내’, ‘가’, ‘어떻게’
, ‘해야’, ‘그대’,
‘를’
, ‘잊을’
,
‘수’ ‘있을’, ‘까’
Tokenizing
05 워드 임베딩(Word Embedding)
Word2vec 예시 ‒ 중심 단어와 주변 단어
Center Word Neighbor Words
‘내’ ‘가’,‘어떻게’
‘가’ ‘내‘, ‘어떻게’, ‘해야’
‘어떻게’ ‘내’, ‘가’, ‘해야‘,‘그대’
‘해야’ ‘가’, ‘어떻게’, ‘그대‘, ‘를’
‘그대’ ‘어떻게’, ‘해야’, ‘를‘,‘잊을’
‘를’ ‘해야’, ‘그대’, ‘잊을‘, ‘수’
‘잊을’ ‘그대’, ‘를’, ‘수’, ‘있을’
‘수’ ‘를’, ‘잊을’, ‘있을’, ‘까’
‘있을＇ ‘잊을’, ‘수’,‘까’
‘까’ ‘수’,‘있을’
‘내’, ‘가’, ‘어떻게, ‘해야’, ‘그대’,
‘를‘
, ‘잊을‘,
‘수’ ‘있을’, ‘까＇
05 워드 임베딩(Word Embedding)
Word2vec 예시 ‒ CBOW 방식
Center Word Neighbor Words
‘내’ ‘가’,‘어떻게’
‘가’ ‘내‘, ‘어떻게’, ‘해야’
‘어떻게’ ‘내’, ‘가’, ‘해야‘,‘그대’
‘해야’ ‘가’, ‘어떻게’, ‘그대‘, ‘를’
‘그대’ ‘어떻게’, ‘해야’, ‘를‘, ‘잊을’
‘를’ ‘해야’, ‘그대’, ‘잊을‘, ‘수’
‘잊을’ ‘그대’, ‘를’, ‘수’, ‘있을’
‘수’ ‘를’, ‘잊을’, ‘있을’, ‘까’
‘있을＇ ‘잊을’, ‘수’, ‘까’
‘까’ ‘수’,‘있을’
OUTPUT INPUT
주변 단어(Context Words)로 중심단어(Center word)를 예측하도록 학습
05 워드 임베딩(Word Embedding)
Word2vec 예시 ‒ Skip-gram 방식
Center Word Neighbor Words
‘내’ ‘가’,‘어떻게’
‘가’ ‘내‘, ‘어떻게’, ‘해야’
‘어떻게’ ‘내’, ‘가’, ‘해야‘,‘그대’
‘해야’ ‘가’, ‘어떻게’, ‘그대‘, ‘를’
‘그대’ ‘어떻게’, ‘해야’, ‘를‘, ‘잊을’
‘를’ ‘해야’, ‘그대’, ‘잊을‘, ‘수’
‘잊을’ ‘그대’, ‘를’, ‘수’, ‘있을’
‘수’ ‘를’, ‘잊을’, ‘있을’, ‘까’
‘있을＇ ‘잊을’, ‘수’, ‘까’
‘까’ ‘수’,‘있을’
INPUT OUTPUT
중심단어(Center word)로 주변 단어(Context Words)를 예측하도록 학습
Confidential all right reserved
순환 신경망(RNN)
06
06 순환 신경망(RNN)
자연어 처리를 위한 딥러닝 모델
자연어 분류
(RNN 계열 모델)
06 순환 신경망(RNN)
MLP 기반 신경망의 자연어 분류 방식
자연어 문장을 기존 MLP 모델에 적용시키기에는 한계가 있음
문서1 : [ [1 0 0 0]
[0 0 1 0]
[0 0 0 1]]
- 오늘
- 밥
- 먹다
[1 0 0 0]
[0 0 1 0]
[0 0 0 1]
�����
�����
Hidden
�����
������
�����
06 순환 신경망(RNN)
자연어 분류를 위한 순환 신경망(Recurrent Neural Network)
RNN
Y
x 입력 노드는 단 하나
(주로 원-핫 벡터 하나 입력)
06 순환 신경망(RNN)
순환 신경망의 입출력 구조
RNN
Y
x
h
출력 값을 두 갈래로 나뉘어
신경망에게 ‘기억’ 하는 기능을 부여
같은 값이나 이름만 다르다
RNN
06 순환 신경망(RNN)
순환 신경망 기반 자연어 분류 예시
Ex) input: [ [수업], [이], [너무], [재밌어] ] label: [1] (0: 부정, 긍정)
[수업이] [이] [너무] [재밌어]
loss 계산
RNN
Y
x
RNN
Y
x
RNN
Y
x
RNN
Y
x
06 순환 신경망(RNN)
순환 신경망 기반 다양한 자연어 처리 기술
Caption: 바다에서 사람이 서핑보드를 타고
있습니다.
Image captioning Chat bot
