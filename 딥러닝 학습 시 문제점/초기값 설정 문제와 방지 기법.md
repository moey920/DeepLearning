# 초기값 설정 문제와 방지 기법

- 잘못된 초기값 설정 – 초기화의 중요성(마찬가지로 sigmiod에서 발생)
    - 초기값에 너무 영향을 받아 초기값에서 벗어나지 못하는 상황이 발생하기도 한다.

## 방지 기법
- **가중치 초기화(Weight Initialization)**
    - 활성화 함수의 입력값이 너무 커지거나 작아지지 않게 만들어주려는 것이 핵심
- 초기화 설정 문제 해결을 위한 Naïve한 방법
    - 표준 정규분포를 이용해 초기화 
    - 표준편차를 0.01로 하는 정규분포로 초기화
        - 표준 정규분포가 양쪽 끝에 몰리는 문제가 발생하여 탄생
- Xavier(자비어) 초기화 방법 + Sigmoid 함수
    - 표준 정규 분포를 입력 개수의 제곱근으로 나누어 줌
    - Sigmoid와 같은 S자 함수의 경우 출력 값들이 정규 분포 형태를 가져야 안정적으로 학습 가능
- Xavier 초기화 방법 + ReLU 함수
    - ReLU 함수에는 Xavier 초기화가 부적합
    - 레이어를 거쳐갈수록 값이 0에 수렴
- He 초기화 방법(RELU)
    - 표준 정규 분포를 입력 개수 절반의 제곱근으로 나누어 줌 `varience = (2/n)^2
    - 10층 레이어에서도 평균과 표준편차가 0 으로 수렴하지 않음

## 적절한 가중치 초기화 방법
    - Sigmoid, tanh의 경우 : Xavier 초기화 방법이 효율적
    - ReLU계의 활성화 함수 사용 시 : Xavier 초기화보다는 He 초기화 방법이 효율적
    - 최근의 대부분의 모델에서는 He 초기화를 주로 선택

### Naive한 가중치 초기화 방법

가중치 초기화 문제는 활성화 함수의 입력값이 너무 커지거나 작아지지 않게 만들어주려는 것이 핵심입니다.

초기화 설정 문제 해결을 위한 Naive한 방법으론 평균이 0, 표준 편차가 1인 표준 정규 분포를 이용해 초기화하는 방법과 평균이 0, 표준 편차가 0.01인 정규분포로 초기화하는 방법이 있습니다.

이번 실습에서는 각 방법으로 가중치를 초기화해보고, 각 경우에 활성화 결괏값들의 분포가 어떻게 변화하는지 확인해봅시다. 실습을 마치고 나면 왜 Naive한 방식으로 가중치를 초기화해서는 안되는지 알 수 있습니다.

표준 정규 분포(평균이 0, 표준 편차가 1인 정규 분포)를 따르는 무작위 데이터를 만들기 위한 함수/라이브러리

`x = np.random.randn(50,20)`  
: 표준 정규 분포를 따르는 무작위 데이터는 위와 같이 만들 수 있습니다. 예시는 20개의 노드를 가진 모델에 들어갈 50개의 데이터입니다. 데이터의 shape은 (50,20)으로, 배열(array) 형태입니다.

모델의 층은 5개가 있으며, 각 층의 노드는 100개씩입니다.

1. 100개의 노드를 가진 모델에 들어갈 1,000개의 입력 데이터 x_1, x_2를 표준 정규 분포를 따르도록 무작위로 생성하세요. 둘은 같은 값입니다.

활성화 함수는 sigmoid 함수이고, 각 층의 활성화 결괏값을 activations_1과 activations_2에 저장합니다.

2. 가중치 w_1은 표준 정규 분포를 따르도록, 가중치 w_2는 평균이 0, 표준 편차가 0.01인 정규 분포를 따르도록 무작위로 생성하세요. 각각은 100개의 노드를 가진 모델에 들어갈 100개의 가중치입니다.

만약 20개의 노드를 가진 모델에 들어갈 50개의 ‘표준편차가 0.3, 평균이 5인 표준 정규 분포를 따르는 무작위 데이터’를 생성하고 싶다면 다음과 같은 코드를 사용할 수 있습니다.

`x = np.random.randn(50, 20) * 0.3 + 5`

3. sigmoid를 통과할 값인 x_1과 w_1의 곱, x_2와 w_2의 곱을 Numpy 메소드를 이용해 각각 a_1, a_2에 정의하세요. x_1과 w_1, x_2와 w_2은 각각 배열이라는 것을 기억하세요.

```
import numpy as np
from visual import *

np.random.seed(100)

def sigmoid(x):
    result = 1 / (1 + np.exp(-x))
    return result

'''
1. 입력 데이터를 정의하세요.

2. 가중치를 정의하세요.

3. sigmoid를 통과할 값인 'a_1', 'a_2'를 정의하세요.
'''

def main():
    
    x_1 = np.random.randn(1000, 100)
    x_2 = np.random.randn(1000, 100)
    
    node_num = 100 # 각 히든 레이어의 노드 개수
    hidden_layer_size = 5 # 히든 레이어의 수
    
    activations_1 = {}
    activations_2 = {}
    
    for i in range(hidden_layer_size):
        if i != 0:
            x_1 = activations_1[i-1]
            x_2 = activations_2[i-1]
        
        w_1 = np.random.randn(100, 100) # 표준 정규 분포
        w_2 = np.random.randn(100, 100) * 0.01 # 평균이 0, 표준 편차가 0.01인 정규 분포
        
        a_1 = np.dot(x_1, w_1)
        a_2 = np.dot(x_2, w_2)
        
        z_1 = sigmoid(a_1)
        z_2 = sigmoid(a_2)
        
        activations_1[i] = z_1
        activations_2[i] = z_2
        
    Visual(activations_1,activations_2)
    
    return activations_1, activations_2

if __name__ == "__main__":
    main()
```

### Xavier 초기화 방법

가중치 초기화의 문제를 해결하기 위해 나온 방법의 하나인 Xavier 초기화 방법은 현재 일반적인 딥러닝 프레임워크들이 표준적으로 이용하고 있습니다.

Xavier 초기화 방법은 앞 레이어의 노드가 n개일 때 표준 편차가 1/root(n)인 분포를 사용하는 것입니다. 즉 표준 정규 분포를 입력 개수의 제곱근으로 나누어주면 됩니다.

따라서 Xavier 초기화 방법을 사용하면 앞 레이어의 노드가 많을수록 다음 레이어의 노드의 초깃값으로 사용하는 가중치가 좁게 퍼집니다.

```
import numpy as np
from visual import *

np.random.seed(100)

def sigmoid(x):
    result = 1 / (1 + np.exp(-x))
    return result

def relu(x):
    result = np.maximum(0,x)
    return result

'''
1. 입력 데이터를 정의하세요.

2. 가중치 초깃값 설정 부분을 왼쪽 설명에 맞게 바꿔보세요.
   Numpy의 연산 메서드를 사용할 수 있습니다.
   
3. sigmoid와 relu를 통과할 값인 'a_sig', 'a_relu'를 정의하세요.
'''

def main():
    
    x_sig = np.random.randn(1000, 100)
    x_relu = np.random.randn(1000, 100)
    
    node_num = 100
    hidden_layer_size = 5
    
    activations_sig = {}
    activations_relu = {}
    
    for i in range(hidden_layer_size):
        if i != 0:
            x_sig = activations_sig[i-1]
            x_relu = activations_relu[i-1]
        
        # 표준편차가 1/ root(n)
        w_sig = np.random.randn(100, 100) * (1 / np.sqrt(node_num))
        w_relu = np.random.randn(100, 100) * (1 / np.sqrt(node_num))
        
        a_sig = np.dot(x_sig, w_sig)
        a_relu = np.dot(x_relu, w_relu)
        
        z_sig = sigmoid(a_sig)
        z_relu = relu(a_relu)
        
        activations_sig[i] = z_sig
        activations_relu[i] = z_relu
        
    Visual(activations_sig, activations_relu)
    
    return activations_sig, activations_relu

if __name__ == "__main__":
    main()
```

### He 초기화 방법

He 초기화 방법은 활성화 함수로 ReLU를 쓸 때 활성화 결괏값들이 한쪽으로 치우치는 문제를 해결하기 위해 나온 방법입니다.

He 초기화 방법은 앞 레이어의 노드가 n개일 때 표준 편차가 root(2) / root(n)인 분포를 사용하는 것입니다. 즉 표준 정규 분포를 입력 개수 절반의 제곱근으로 나누어주면 됩니다.

Xavier 초기화 방법은 표준 편차가 1/root(n)이라고 하였습니다. ReLU는 음의 영역에 대한 함숫값이 0이라서 더 넓게 분포시키기 위해 root(2) 배의 계수가 필요하다고 이해할 수 있습니다.

```
import numpy as np
from visual import *

np.random.seed(100)
    
def relu(x):
    result = np.maximum(0,x)
    return result

'''
1. 입력 데이터를 정의하세요.

2. 가중치 초깃값 설정 부분을 왼쪽 설명에 맞게 바꿔보세요.
   Numpy의 연산 메서드를 사용할 수 있습니다.
   
3. relu를 통과할 값인 'a_relu'를 정의하세요.
'''

def main():
    
    x_relu = np.random.randn(1000, 100)
    
    node_num = 100
    hidden_layer_size = 5
    
    activations_relu = {}
    
    for i in range(hidden_layer_size):
        if i != 0:
            x_relu = activations_relu[i-1]
            
        w_relu = np.random.randn(100, 100) * np.sqrt(2/node_num)
        
        a_relu = np.dot(x_relu, w_relu)
        
        z_relu = relu(a_relu)
        
        activations_relu[i] = z_relu
        
    Visual(activations_relu)
    
    return activations_relu    

if __name__ == "__main__":
    main()
```

