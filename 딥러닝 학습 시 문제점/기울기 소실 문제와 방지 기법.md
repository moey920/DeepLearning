기울기 소실 문제와 방지 기법
03
03 기울기 소실 문제와 방지 기법
딥러닝 모델 학습 방법 되짚어보기
Σ
perceptron
Σ
perceptron
1
Σ
perceptron
…
𝑥1 = 1
𝑥2 = −1
𝑤1 = 2
𝑤2 = 1
0.73
0.5
𝑤5 = 1
𝑤6 = −1
0.23 0.55 𝑤6 = 2
𝑤6의 gradient를 구하기 위해선
그 전단의 계산 값이 필요
나의 목표 target 값과 실제 모델이 예측한 output 값이 얼마나 차이 나는지 구한 후
오차값을 다시 뒤로 전파해가며 변수들을 갱신하는 알고리즘
03 기울기 소실 문제와 방지 기법
기울기 소실 문제(Vanishing Gradient)의 발생 원인
기울기가 0인 값을 전달하며 중간 전달값이 사라지는 문제
03 기울기 소실 문제와 방지 기법
기울기 소실 문제
기울기가 소실되는 문제가 반복되며 학습이 잘 이루어지지 않음
03 기울기 소실 문제와 방지 기법
기울기 소실 문제 해결 방법 : ReLU
ReLU function
활성화 함수(Activation Function) 방식 변화
기존에 사용하던 sigmoid 함수 대신 ReLU 함수를 사용하여 해결
03 기울기 소실 문제와 방지 기법
기울기 소실 문제 해결 방법 : Tanh
Tanh
내부 Hidden Layer에는 ReLU를 적용하고
Output Layer에서만 Tanh를 적용
Confidential all right reserved
초기값 설정 문제와 방지 기법
04
04 초기값 설정 문제와 방지 기법
잘못된 초기값 설정 – 초기화의 중요성
0.01 Σ
50
𝑤 = 100
𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛 𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛
𝑏𝑖𝑎𝑠
1
51
초기화
Gradient = 0.00...01
51
04 초기값 설정 문제와 방지 기법
가중치 초기화(Weight Initialization)
활성화 함수의 입력값이 너무 커지거나
작아지지 않게 만들어주려는 것이 핵심
𝑥 Σ
𝑏
𝑤
𝒂𝒄𝒕𝒊𝒗𝒂𝒕𝒊𝒐𝒏 𝒇𝒖𝒏𝒄𝒕𝒊𝒐𝒏
𝑏𝑖𝑎𝑠
1
51
초기화
Gradient = 0.00...01
04 초기값 설정 문제와 방지 기법
초기화 설정 문제 해결을 위한 Naïve한 방법
표준 정규분포를 이용해 초기화 표준편차를 0.01로 하는 정규분포로 초기화
04 초기값 설정 문제와 방지 기법
Xavier 초기화 방법 + Sigmoid 함수
표준 정규 분포를 입력 개수의 제곱근으로 나누어 줌
Sigmoid와 같은 S자 함수의 경우 출력 값들이
정규 분포 형태를 가져야 안정적으로 학습 가능
04 초기값 설정 문제와 방지 기법
Xavier 초기화 방법 + ReLU 함수
ReLU 함수에는 Xavier 초기화가 부적합
레이어를 거쳐갈수록 값이 0에 수렴
ReLU function
04 초기값 설정 문제와 방지 기법
He 초기화 방법
표준 정규 분포를 입력 개수 절반의 제곱근으로 나누어 줌
ReLU 함수와 He 초기화 방법을 사용했을 경우의 그래프는 위와 같음
10층 레이어에서도 평균과 표준편차가 0 으로 수렴하지 않음
04 초기값 설정 문제와 방지 기법
적절한 가중치 초기화 방법
Sigmoid, tanh의 경우
Xavier 초기화 방법이 효율적
ReLU계의 활성화 함수 사용 시
Xavier 초기화보다는 He 초기화 방법이 효율적
최근의 대부분의 모델에서는 He 초기화를 주로 선택
Confidential all right reserved
과적합 문제와 방지 기법
05
05 과적합과 방지 기법
딥러닝 모델 학습에서의 과적합 방지 기법
다양한 과적합 현상 방지 기법 등장
정규화(Regularization)
드롭아웃(Dropout)
배치 정규화(Batch Normalization)
모델이 복잡해질수록
parameter들은 많아지고,
절댓값이 커지는 경향이 발생함
기존 손실함수에 규제항을 더해 최적값 찾기 가능
05 과적합 문제와 방지 기법
정규화(Regularization)
05 과적합 문제와 방지 기법
L1 정규화(Lasso Regularization)
가중치의 절댓값의 합을 규제항으로 정의
작은 가중치들이 거의 0으로 수렴하여 몇개의 중요한 가중치들만 남음
05 과적합 문제와 방지 기법
L2 정규화(Ridge Regularization)
가중치의 제곱의 합을 규제항으로 정의
L1 정규화에 비하여 0으로 수렴하는 가중치가 적음
큰 값을 가진 가중치를 더욱 제약하는 효과
05 과적합 문제와 방지 기법
드롭아웃(DropOut)
드롭아웃 적용
각 layer마다 일정 비율의 뉴런을 임의로 drop시켜 나머지 뉴런들만 학습하는 방법
드롭아웃을 적용하면 학습되는 노드와 가중치들이 매번 달라짐
05 과적합 문제와 방지 기법
드롭아웃의 적용 방법
다른 정규화 기법들과 상호 보완적으로 사용 가능
drop된 뉴런은 backpropagation 때 신호를 차단
Test 때는 모든 뉴런에 신호를 전달
05 과적합 문제와 방지 기법
배치 정규화(Batch Normalization)
∑
Fully connected
Batch Normalization
Activation function
Normalization을 처음 Input data 뿐만 아니라
신경망 내부 Hidden Layer의 input에도 적용
05 과적합 문제와 방지 기법
배치 정규화의 장점
매 Layer마다 정규화를 진행하므로
가중치 초기값에 크게 의존하지 않음(초기화 중요도 감소)
과적합 억제(Dropout, L1,L2 정규화 필요성 감소)
핵심은 학습 속도의 향상
