# 기울기 소실(Vanishing Gradient) 문제와 방지 기법

- 이후 전단의 gradient를 구하기 위해선 그 전단의 계산 값이 필요
- 나의 목표 target 값과 실제 모델이 예측한 output 값이 얼마나 차이 나는지 구한 후 오차값을 다시 뒤로 전파해가며 변수들을 갱신하는 알고리즘

## 기울기 소실 문제(Vanishing Gradient)의 발생 원인
- 기울기가 0인 값을 전달하며 중간 전달값이 사라지는 문제
    - Sigmoid(logistic) function은 0~1사이의 결과를 내는데, 중간에 기울기가 가장 크고, 양 옆은 기울기가 0에 수렴한다.
    - 입력값의 절댓값이 클수록 해당 입력값에 대한 기울기가 0에 가까워진다.
    - 기울기가 소실되는 문제가 반복되며 학습이 잘 이루어지지 않음

## 기울기 소실 문제와 방지 기법
1. 기울기 소실 문제 해결 방법 : ReLU
    - `ReLU function`
        - ReLU(x) = max(0, x)
        - R'(y) = 1(x>=0) or 0(x<0)
    - **활성화 함수(Activation Function) 방식 변화**
        - 기존에 사용하던 sigmoid 함수 대신 ReLU 함수를 사용하여 해결

2. 기울기 소실 문제 해결 방법 : Tanh

- 내부 Hidden Layer에는 **ReLU**를 적용하고 Output Layer에서만 **Tanh**를 적용
    - `Tanh function`
        - Tanh(x) = e^2x+1 / e^2x-1
        - T'(x) = 1-tanh^2(x)

### 기울기 소실 문제(Vanishing Gradient) 확인하기

역전파(back propagation) 알고리즘이란 우리의 목푯값과 실제 모델이 예측한 예측값이 얼마나 차이 나는지 구한 후, 오차값을 다시 뒤로 전파해가며 가중치(weight)들을 업데이트하는 과정이라고 배웠습니다.

그러나 깊은 층의 모델에선 역전파 시에 전달되는 손실 함수(loss function)의 gradient 값에 활성화 함수인 sigmoid 함수의 0에 가까운 기울기 값이 계속해서 곱해지면서 결국 가중치 업데이트가 잘 안 되는 문제가 생기는데, 이것이 바로 기울기 소실 문제(Vanishing Gradient)입니다.

이번 실습에서는 모델의 층이 깊은 경우 히든층의 활성화 함수가 ‘relu’일 때와 ‘sigmoid’일 때의 모델 정확도를 확인해보고, 왜 최근에는 활성화 함수로 sigmoid를 잘 쓰지 않는지 직접 확인해봅시다.

#### MNIST Dataset

실습에서 활용할 데이터셋은 28x28의 크기를 가진 MNIST 손글씨 데이터셋입니다. MNIST 데이터는 0 ~ 9까지의 손글씨 이미지로 구성되어 있으며 데이터의 label은 0 ~ 9의 정수로 이루어져 있습니다. 이번 실습에선 MNIST 데이터를 모델의 입력층에서 1차원 데이터로 변환해 사용합니다.

- 결과(accuracy)
```
accuracy_relu:  0.8826
# 기울기 소실로 인하여 sigmoid 모델은 학습이 제대로 안된 것을 확인할 수 있다.
accuracy_sig:  0.2058
```

```
import tensorflow as tf

import logging, os
logging.disable(logging.WARNING)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

'''
1. 활성화 함수는 출력층만 그대로 두고 
   나머지 히든층들은 `relu`로 설정하세요.
'''

def make_model_relu():
    
    model_relu = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    return model_relu
    
'''
2. 활성화 함수는 출력층만 그대로 두고
   나머지 히든층들은 `sigmoid`로 설정하세요.
'''
    
def make_model_sig():
    
    model_sig = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(10, activation='sigmoid'),
        tf.keras.layers.Dense(10, activation='sigmoid'),
        tf.keras.layers.Dense(10, activation='sigmoid'),
        tf.keras.layers.Dense(10, activation='sigmoid'),
        tf.keras.layers.Dense(10, activation='sigmoid'),
        tf.keras.layers.Dense(10, activation='sigmoid'),
        tf.keras.layers.Dense(10, activation='sigmoid'),
        tf.keras.layers.Dense(10, activation='sigmoid'),
        tf.keras.layers.Dense(10, activation='sigmoid'),
        tf.keras.layers.Dense(10, activation='sigmoid'),
        tf.keras.layers.Dense(10, activation='sigmoid'),
        tf.keras.layers.Dense(10, activation='sigmoid'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    return model_sig

'''
3. 두 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.

   Step01. model_relu와 model_sig 불러옵니다.
   
   Step02. 두 모델의 최적화 방법과 손실 함수를 
           똑같이 설정합니다.
   
   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.
           우리가 만든 모델이 얼마나 깊은지 확인해보세요.
   
   Step04. 두 모델을 학습시킵니다. 
           'epochs'는 5로 설정합니다.
           검증용 데이터는 설정하지 않습니다. 
           'verbose'는 0으로 설정합니다.
   
   Step05. 두 모델을 테스트하고 점수를 출력합니다. 
           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.
'''

def main():
    
    # MNIST 데이터를 불러오고 전처리합니다.
    mnist = tf.keras.datasets.mnist
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    
    model_relu = make_model_relu()  # 히든층들의 활성화 함수로 relu를 쓰는 모델입니다.
    model_sig = make_model_sig()   # 히든층들의 활성화 함수로 sigmoid를 쓰는 모델입니다.
    
    model_relu.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    model_sig.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    
    None
    None
    
    model_relu_history = model_relu.fit(x_train, y_train, epochs = 5, verbose=0)
    print('\n')
    model_sig_history = model_sig.fit(x_train, y_train, epochs = 5, verbose=0)
    
    scores_relu = model_relu.evaluate(x_test, y_test)
    scores_sig = model_sig.evaluate(x_test, y_test)
    
    print('\naccuracy_relu: ', scores_relu[-1])
    print('accuracy_sig: ', scores_sig[-1])
    
    return model_relu_history, model_sig_history

if __name__ == "__main__":
    main()
```

### 활성화 함수 다르게 적용하기

이번 실습에서는 sigmoid의 기울기 소실 문제를 해결하기 위해 등장한 활성화 함수인 relu와 tanh를 활용해봅시다. 동일한 모델에서 히든층의 활성화 함수가 각각 ‘sigmoid’, ‘relu’, ‘tanh’일 때의 모델 정확도를 확인해봅시다.

- 결과 :
```
# 은닉층에 sigmoid 활성화 함수를 썼을 때만 학습이 제대로 되지 않을 것을 알 수 있다.
accuracy_sig:  0.1028
accuracy_relu:  0.9557
accuracy_tanh:  0.9611
```

```
import tensorflow as tf

import logging, os
logging.disable(logging.WARNING)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
    
'''
1. 활성화 함수는 출력층만 그대로 두고 
   나머지 히든층들은 'sigmoid'로 설정하세요.
'''
    
def make_model_sig():
    
    model_sig = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(64, activation='sigmoid'),
        tf.keras.layers.Dense(64, activation='sigmoid'),
        tf.keras.layers.Dense(64, activation='sigmoid'),
        tf.keras.layers.Dense(64, activation='sigmoid'),
        tf.keras.layers.Dense(64, activation='sigmoid'),
        tf.keras.layers.Dense(64, activation='sigmoid'),
        tf.keras.layers.Dense(32, activation='sigmoid'),
        tf.keras.layers.Dense(32, activation='sigmoid'),
        tf.keras.layers.Dense(32, activation='sigmoid'),
        tf.keras.layers.Dense(32, activation='sigmoid'),
        tf.keras.layers.Dense(32, activation='sigmoid'),
        tf.keras.layers.Dense(32, activation='sigmoid'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    return model_sig
    
'''
2. 활성화 함수는 출력층만 그대로 두고 
   나머지 히든층들은 'relu'로 설정하세요.
'''

def make_model_relu():
    
    model_relu = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    return model_relu

    
'''
3. 활성화 함수는 출력층만 그대로 두고 
   나머지 히든층들은 'tanh'로 설정하세요.
'''
    
def make_model_tanh():
    
    model_tanh = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(64, activation='tanh'),
        tf.keras.layers.Dense(64, activation='tanh'),
        tf.keras.layers.Dense(64, activation='tanh'),
        tf.keras.layers.Dense(64, activation='tanh'),
        tf.keras.layers.Dense(64, activation='tanh'),
        tf.keras.layers.Dense(64, activation='tanh'),
        tf.keras.layers.Dense(32, activation='tanh'),
        tf.keras.layers.Dense(32, activation='tanh'),
        tf.keras.layers.Dense(32, activation='tanh'),
        tf.keras.layers.Dense(32, activation='tanh'),
        tf.keras.layers.Dense(32, activation='tanh'),
        tf.keras.layers.Dense(32, activation='tanh'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    return model_tanh

'''
4. 세 개의 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.

   Step01. make_model_sig, make_model_relu, make_model_tanh 함수를 이용해 세 모델을 불러옵니다.
   
   Step02. 세 모델의 손실 함수, 최적화 알고리즘, 
          평가 방법을 설정합니다.
   
   Step03. 세 모델의 구조를 확인하는 코드를 작성합니다.
           우리가 만든 모델이 얼마나 깊은지 확인해보세요.
   
   Step04. 세 모델을 학습시킵니다. 
           'epochs'는 5로 설정합니다.
           검증용 데이터는 설정하지 않습니다.
   
   Step05. 세 모델을 테스트하고 accuracy 값을 출력합니다. 
           셋 중 어느 모델의 성능이 가장 좋은지 확인해보세요.
'''

def main():
    
    # MNIST 데이터를 불러오고 전처리합니다.
    mnist = tf.keras.datasets.mnist
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    
    model_sig = make_model_sig()   # 히든층들의 활성화 함수로 sigmoid를 쓰는 모델입니다.
    model_relu = make_model_relu()  # 히든층들의 활성화 함수로 relu를 쓰는 모델입니다.
    model_tanh = make_model_tanh()  # 히든층들의 활성화 함수로 tanh를 쓰는 모델입니다.
    
    model_sig.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    model_relu.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    model_tanh.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    
    model_sig_history = model_sig.fit(x_train, y_train, epochs = 5, batch_size = 500, verbose = 0)
    print('\n')
    model_relu_history = model_relu.fit(x_train, y_train, epochs = 5, batch_size = 500, verbose = 0)
    print('\n')
    model_tanh_history = model_tanh.fit(x_train, y_train, epochs = 5, batch_size = 500, verbose = 0)
    
    scores_sig = model_sig.evaluate(x_test, y_test)
    scores_relu = model_relu.evaluate(x_test, y_test)
    scores_tanh = model_tanh.evaluate(x_test, y_test)
    
    print('\naccuracy_sig: ', scores_sig[-1])
    print('accuracy_relu: ', scores_relu[-1])
    print('accuracy_tanh: ', scores_tanh[-1])
    
    return model_sig_history, model_relu_history, model_tanh_history

if __name__ == "__main__":
    main()
```

