# 기울기 소실(Vanishing Gradient) 문제와 방지 기법

- 이후 전단의 gradient를 구하기 위해선 그 전단의 계산 값이 필요
- 나의 목표 target 값과 실제 모델이 예측한 output 값이 얼마나 차이 나는지 구한 후 오차값을 다시 뒤로 전파해가며 변수들을 갱신하는 알고리즘

## 기울기 소실 문제(Vanishing Gradient)의 발생 원인
- 기울기가 0인 값을 전달하며 중간 전달값이 사라지는 문제
    - Sigmoid(logistic) function은 0~1사이의 결과를 내는데, 중간에 기울기가 가장 크고, 양 옆은 기울기가 0에 수렴한다.
    - 양 옆으로 갈수록 Gradient가 급격히 떨어지면서, 0에 가까운 값을 곱해야한다. 
    - 기울기가 소실되는 문제가 반복되며 학습이 잘 이루어지지 않음

## 기울기 소실 문제와 방지 기법
1. 기울기 소실 문제 해결 방법 : ReLU
    - `ReLU function`
        - ReLU(x) = max(0, x)
        - R'(y) = 1(x>=0) or 0(x<0)
    - **활성화 함수(Activation Function) 방식 변화**
        - 기존에 사용하던 sigmoid 함수 대신 ReLU 함수를 사용하여 해결

2. 기울기 소실 문제 해결 방법 : Tanh

- 내부 Hidden Layer에는 **ReLU**를 적용하고 Output Layer에서만 **Tanh**를 적용
    - `Tanh function`
        - Tanh(x) = e^2x+1 / e^2x-1
        - T'(x) = 1-tanh^2(x)




