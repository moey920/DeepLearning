# 과적합 문제(Overfitting)와 방지 기법

- 딥러닝 모델 학습에서의 과적합 방지 기법
    - 정규화(Regularization)
    - 드롭아웃(Dropout)
    - 배치 정규화(Batch Normalization)

- 모델이 복잡해질수록 parameter들은 많아지고, 절댓값이 커지는 경향이 발생함
    - x^1 + x^2 + .. + x^20 이라는 문제가 있다고 가정할 때, 계수가 0에 가까운 것은 학습하지 않는 방법으로 정규화한다. 이런 방법으로 과적합을 방지한다.(일반 식에서의 정규화) => w값이 0에 가까우면 0으로 만들어 모델에서 없앤다.(딥러닝 Regularization)
    - 기존 손실함수에 **규제항**을 더해 최적값 찾기 가능

## 과적합 문제와 방지 기법

- 정규화(Regularization)
    - L1 정규화(Lasso Regularization)
        - 가중치의 **절댓값**의 합을 규제항으로 정의 `∑(|W|)`
        - 작은 가중치들이 거의 0으로 수렴하여 몇 개의 중요한 가중치들만 남음

    - L2 정규화(Ridge Regularization)
        - 가중치의 **제곱의 합**을 규제항으로 정의 `∑(W^2)`
        - L1 정규화에 비하여 0으로 수렴하는 가중치가 적음
        - 큰 값을 가진 가중치를 더욱 제약하는 효과

    - 드롭아웃(DropOut)
        - 각 layer마다 **일정 비율의 뉴런을 임의로 drop**시켜 나머지 뉴런들만 학습하는 방법(모델의 복잡도를 내려 과적합 방지)
        - 드롭아웃을 적용하면 학습되는 노드와 가중치들이 매번 달라짐
        - 드롭아웃의 적용 방법
            - 다른 정규화 기법들과 상호 보완적으로 사용 가능(L1, L2와 같이 사용한다.)
            - drop된 뉴런은 backpropagation 때 신호를 차단
            - Test 때는 모든 뉴런에 신호를 전달
    - 배치 정규화(Batch Normalization)
        - Features의 range가 각각 다를 수 있다, (0~100, 0~1, -3~3 등)
            - 모델이 잘 학습되지 않는다. -> mean=0, var=1 등 정규화를 하면 학습이 더 잘된다.
        - Normalization을 처음 Input data 뿐만 아니라 신경망 내부 Hidden Layer의 input에도 적용
        - 배치 정규화의 장점
            - 매 Layer마다 정규화를 진행하므로 가중치 초기값에 크게 의존하지 않음(초기화 중요도 감소)
        - 과적합 억제(Dropout, L1,L2 정규화 필요성 감소)
        - 핵심은 **학습 속도의 향상**

### 과적합(Overfitting)

과적합(Overfitting)은 모델이 학습 데이터에만 너무 치중되어 학습 데이터에 대한 예측 성능은 좋으나 테스트 데이터에 대한 예측 성능이 떨어지는 경우를 말합니다.

모델이 과적합 되면 일반화되지 않은 모델이라고도 합니다. 과적합이 발생하는 원인은 아래와 같습니다.

- 데이터의 퍼진 정도, 즉 분산(variance)이 높은 경우
- 너무 많이 학습 데이터를 학습시킨 경우 (epochs가 매우 큰 경우)
- 학습에 사용된 파라미터가 너무 많은 경우
- 데이터에 비해 모델이 너무 복잡한 경우
- 데이터에 노이즈 & 이상치(outlier)가 너무 많은 경우

이번 실습에서는 일부러 과적합 된 모델을 만들어 보고 손실값(loss) 그래프를 통해 기존 모델과 어떻게 성능 차이가 나는지 확인해보겠습니다.

- 결과
```
scores_basic:  0.7481468
scores_overfit:  3.8213732
```

```
import numpy as np
import tensorflow as tf
from visual import *

import logging, os
logging.disable(logging.WARNING)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

# 데이터를 전처리하는 함수

def sequences_shaping(sequences, dimension):
    
    results = np.zeros((len(sequences), dimension))
    for i, word_indices in enumerate(sequences):
        results[i, word_indices] = 1.0 
        
    return results
    
'''
1. 과적합 될 모델과 비교하기 위해 기본 모델을 
   마크다운 설명과 동일하게 생성합니다.
'''

def Basic(word_num):
    
    basic_model = tf.keras.Sequential([
        tf.keras.layers.Dense(256, activation = 'relu', input_shape=(word_num,)),
        tf.keras.layers.Dense(128, activation = 'relu'),
        tf.keras.layers.Dense(1, activation= 'sigmoid')
    ])
    
    return basic_model

'''
2. 기본 모델의 레이어 수와 노드 수를 자유롭게 늘려서
   과적합 될 모델을 생성합니다.
'''

def Overfitting(word_num):
    
    overfit_model = tf.keras.Sequential([
        tf.keras.layers.Dense(1024, activation = 'relu', input_shape=(word_num,)),
        tf.keras.layers.Dense(512, activation = 'relu'),
        tf.keras.layers.Dense(128, activation = 'relu'),
        tf.keras.layers.Dense(64, activation = 'relu'),
        tf.keras.layers.Dense(64, activation = 'relu'),
        tf.keras.layers.Dense(1, activation= 'sigmoid')
    ])
    
    return overfit_model

'''
3. 두 개의 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.

   Step01. basic_model와 overfit_model 함수를 이용해 
           두 모델을 불러옵니다.
   
   Step02. 두 모델의 손실 함수, 최적화 알고리즘, 
           평가 방법을 설정합니다.
   
   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.
   
   Step04. 두 모델을 학습시킵니다. 
           검증용 데이터도 설정해주세요.
           
           기본 모델은 'epochs'를 20, 
           과적합 모델은 'epochs'를 300이상으로 설정합니다.
           'batch_size'는 두 모델 모두 500으로 설정합니다.
   
   Step05. 두 모델을 테스트하고 
           binary crossentropy 값을 출력합니다. 
'''

def main():
    
    word_num = 100
    data_num = 25000
    
    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.
    
    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)
    
    train_data = sequences_shaping(train_data, dimension = word_num)
    test_data = sequences_shaping(test_data, dimension = word_num)
    
    basic_model = Basic(word_num)    # 기본 모델입니다.
    overfit_model = Overfitting(word_num)  # 과적합시킬 모델입니다.
    
    basic_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'binary_crossentropy'])
    overfit_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'binary_crossentropy'])
    
    basic_history = basic_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, validation_data = (test_data, test_labels))
    print('\n')
    overfit_history = overfit_model.fit(train_data, train_labels, epochs = 150, batch_size = 1000, validation_data = (test_data, test_labels))
    
    scores_basic = basic_model.evaluate(test_data, test_labels)
    scores_overfit = overfit_model.evaluate(test_data, test_labels)
    
    print('\nscores_basic: ', scores_basic[-1])
    print('scores_overfit: ', scores_overfit[-1])
    
    Visulaize([('Basic', basic_history),('Overfitting', overfit_history)])
    
    return basic_history, overfit_history

if __name__ == "__main__":
    main()
```

### L1, L2 정규화(Regularization) 기법

- L1 정규화 : 가중치(weight)의 절댓값에 비례하는 손실(loss)이 기존 손실 함수(loss function)에 추가되는 형태입니다.

`Total Loss = Loss + λ∑|W|`

L1 정규화는 모델 내의 **일부 가중치를 0으로 만들어 의미 있는 가중치만 남도록** 만들어줍니다. 이를 통해 모델을 일반화시킬 수 있습니다. 다른 말로 **Sparse Model**을 만든다라고도 합니다.

- L1 정규화를 적용하기 위해 사용하는 함수/라이브러리
    - `tf.keras.layers.Dense(kernel_regularizer = tf.keras.regularizers.l1(ratio))`
    - ratio : 가중치에 L1 정규화를 적용하는 비율 (0.001 ~0.005)

- L2 정규화 : 가중치의 제곱에 비례하는 손실이 기존 손실 함수에 추가되는 형태입니다.

`Total Loss = Loss + λ∑W^2`
 
L2 정규화는 학습이 진행될 때 가중치의 값이 0에 가까워지도록 만들어줍니다. 가중치를 0으로 만들어주는 L1 정규화와는 차이가 있습니다.

이를 통해 **특정 가중치에 치중되지 않도록 가중치 값을 조율**하게 되며 **가중치 감쇠 (Weight Decay)**라고도 부릅니다.

- L2 정규화를 적용하기 위해 사용하는 함수/라이브러리
    - `tf.keras.layers.Dense(kernel_regularizer = tf.keras.regularizers.l2(ratio))`
    - ratio : 가중치에 L2 정규화를 적용하는 비율 (0.001 ~0.005)

- 결과 : L1, L2 정규화를 이용했을 때가 그렇지 않을 때보다 성능이 좋다는 것을 확인할 수 있습니다.
```
scores_basic:  0.6323735
scores_l1:  0.5747952
scores_l2:  0.57645565
```

```
import numpy as np
import tensorflow as tf
from visual import *

import logging, os
logging.disable(logging.WARNING)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

# 데이터를 전처리하는 함수

def sequences_shaping(sequences, dimension):
    
    results = np.zeros((len(sequences), dimension))
    for i, word_indices in enumerate(sequences):
        results[i, word_indices] = 1.0 
    
    return results

'''
1. L1, L2 정규화를 적용한 모델과 비교하기 위한
   하나의 기본 모델을 자유롭게 생성합니다.
'''

def Basic(word_num):
    
    basic_model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, input_shape = (word_num,), activation = 'relu'),
        tf.keras.layers.Dense(128, activation = 'relu'),
        tf.keras.layers.Dense(1, activation = 'sigmoid')
    ])
    
    return basic_model

'''
2. 기본 모델에 L1 정규화를 적용합니다.
   입력층과 히든층에만 적용하세요.
'''

def L1(word_num):
    
    l1_model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, input_shape = (word_num,), activation = 'relu', kernel_regularizer = tf.keras.regularizers.l1(0.001)),
        tf.keras.layers.Dense(128, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l1(0.001)),
        tf.keras.layers.Dense(1, activation = 'sigmoid')
    ])
    
    return l1_model

'''
3. 기본 모델에 L2 정규화를 적용합니다.
   입력층과 히든층에만 적용하세요.
'''

def L2(word_num):
    
    l2_model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, input_shape = (word_num,), activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.Dense(128, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.Dense(1, activation = 'sigmoid')
    ])
    
    return l2_model


'''
4. 세 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.

   Step01. Basic, L1, L2 함수를 이용해 세 모델을 불러옵니다.
   
   Step02. 세 모델의 손실 함수, 최적화 알고리즘, 
           평가 방법을 설정합니다.
   
   Step03. 세 모델의 구조를 확인하는 코드를 작성합니다.
   
   Step04. 세 모델을 학습시킵니다. 
           세 모델 모두 'epochs'는 20,
           'batch_size'는 500으로 설정합니다. 
           검증용 데이터도 설정해주세요.
   
   Step05. 세 모델을 테스트하고 
           binary crossentropy 값을 출력합니다. 
           셋 중 어느 모델의 성능이 가장 좋은지 확인해보세요.
'''

def main():
    
    word_num = 100
    data_num = 25000
    
    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.
    
    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)
    
    train_data = sequences_shaping(train_data, dimension = word_num)
    test_data = sequences_shaping(test_data, dimension = word_num)
    
    basic_model = Basic(word_num)  # 기본 모델입니다.
    l1_model = L1(word_num)     # L1 정규화를 적용할 모델입니다.
    l2_model = L2(word_num)     # L2 정규화를 적용할 모델입니다.
    
    basic_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'binary_crossentropy'])
    l1_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'binary_crossentropy'])
    l2_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'binary_crossentropy'])
    
    basic_history = basic_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, validation_data = (test_data, test_labels))
    print('\n')
    l1_history = l1_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, validation_data = (test_data, test_labels))
    print('\n')
    l2_history = l2_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, validation_data = (test_data, test_labels))
    
    scores_basic = basic_model.evaluate(test_data, test_labels)
    scores_l1 = l1_model.evaluate(test_data, test_labels)
    scores_l2 = l2_model.evaluate(test_data, test_labels)
    
    print('\nscores_basic: ', scores_basic[-1])
    print('scores_l1: ', scores_l1[-1])
    print('scores_l2: ', scores_l2[-1])
    
    Visulaize([('Basic', basic_history),('L1 Regularization', l1_history), ('L2 Regularization', l2_history)])
    
    return basic_history, l1_history, l2_history

if __name__ == "__main__":
    main()              
```

### 드롭아웃(Dropout) 기법

드롭아웃(DropOut)은 모델이 과적합되는 것을 막기 위한 가장 보편적인 정규화(Regularization) 기법의 하나입니다.

드롭아웃은 데이터를 학습할 때, 일부 퍼셉트론(뉴런)을 랜덤하게 0으로 만들어 모델 내부의 특정 가중치(Weight)에 치중되는 것을 막습니다.

이를 통해 모델이 일부 데이터에 가중되는 것을 막고 일반화된 모델을 만들 수 있습니다.

드롭아웃을 사용하는 데 있어 주의할 점은 학습이 끝난 후 **테스트 과정에서는 드롭아웃을 사용하면 안된다**는 점입니다.

이번 실습에선 드롭아웃을 적용한 모델과 적용하지 않은 모델의 차이를 보겠습니다.

- 드롭아웃을 사용하기 위한 함수/라이브러리 : `tf.keras.layers.Dropout(prob)`
    - prob : 드롭아웃을 적용할 확률 (0.1 ~ 0.5)

- 결과 : Dropout을 적용한 모델이 성능이 더 좋은 것을 확인할 수 있습니다.
```
scores_basic:  0.63617074
scores_dropout:  0.57942736
```

```
import numpy as np
import tensorflow as tf
from visual import *

import logging, os
logging.disable(logging.WARNING)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

# 데이터를 전처리하는 함수

def sequences_shaping(sequences, dimension):
    
    results = np.zeros((len(sequences), dimension))
    for i, word_indices in enumerate(sequences):
        results[i, word_indices] = 1.0 
        
    return results
    
'''
1. 드롭아웃을 적용할 모델과 비교하기 위한
   하나의 기본 모델을 자유롭게 생성합니다.
'''

def Basic(word_num):
    
    basic_model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, input_shape = (word_num, ), activation = 'relu'),
        tf.keras.layers.Dense(64, activation = 'relu'),
        tf.keras.layers.Dense(1, activation = 'sigmoid')
    ])
    
    return basic_model
    
'''
2. 기본 모델에 드롭아웃 레이어를 추가합니다.
   일반적으로 마지막 히든층과 출력층 사이에 하나만 추가합니다.
   드롭아웃 적용 확률은 자유롭게 설정하세요.
'''

def Dropout(word_num):
    
    dropout_model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, input_shape = (word_num, ), activation = 'relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(64, activation = 'relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(1, activation = 'sigmoid')
    ])
    
    return dropout_model

'''
3. 두 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.

   Step01. Basic, Dropout 함수를 이용해 두 모델을 불러옵니다.
   
   Step02. 두 모델의 손실 함수, 최적화 알고리즘, 
           평가 방법을 설정합니다.
   
   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.
   
   Step04. 두 모델을 학습시킵니다. 
           두 모델 모두 'epochs'는 20,
           'batch_size'는 500으로 설정합니다. 
           검증용 데이터도 설정해주세요.
   
   Step05. 두 모델을 테스트하고 
           binary crossentropy 점수를 출력합니다. 
           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.
'''


def main():
    
    word_num = 100
    data_num = 25000
    
    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.
    
    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)
    
    train_data = sequences_shaping(train_data, dimension = word_num)
    test_data = sequences_shaping(test_data, dimension = word_num)
    
    basic_model = Basic(word_num)    # 기본 모델입니다.
    dropout_model = Dropout(word_num)  # 드롭아웃을 적용할 모델입니다.
    
    basic_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'binary_crossentropy'])
    dropout_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'binary_crossentropy'])
    
    
    basic_history = basic_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, verbose = 0, validation_data = (test_data, test_labels))
    print('\n')
    dropout_history = dropout_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, verbose = 0, validation_data = (test_data, test_labels))
    
    scores_basic = basic_model.evaluate(test_data, test_labels)
    scores_dropout = dropout_model.evaluate(test_data, test_labels)
    
    print('\nscores_basic: ', scores_basic[-1])
    print('scores_dropout: ', scores_dropout[-1])
    
    Visulaize([('Basic', basic_history),('Dropout', dropout_history)])
    
    return basic_history, dropout_history

if __name__ == "__main__":
    main()
```

### 배치 정규화(Batch Normalization)

배치 정규화(Batch Normalization)는 정규화를 모델에 들어가는 입력 데이터뿐만 아니라 모델 내부 히든층의 입력 노드에도 적용하는 것입니다.

배치 정규화를 적용하면 층마다 정규화를 진행하므로 가중치 초깃값에 크게 의존하지 않습니다. 즉, 가중치 초기화의 중요도가 감소합니다.

또한 과적합을 억제합니다. 즉, 드롭아웃(Dropout)과 L1, L2 정규화의 필요성이 감소합니다.

가장 큰 장점은 학습 속도도 빨라진다는 것입니다.

- 배치 정규화를 사용하기 위한 함수/라이브러리 : 배치 정규화는 하나의 레이어로써 Dense 레이어와 활성화 함수 사이에서 작용합니다.

따라서 이번 실습에서는 기본 모델을 생성할 때 **활성화 함수와 똑같은 역할을 하는 Activation 레이어를 따로 활용**해야 합니다.

- `tf.keras.layers.BatchNormalization()`
    - 배치 정규화를 하는 레이어입니다.
- `tf.keras.layers.Activation()`
    - 활성화 함수를 추가하는 레이어입니다.

활성화 함수 레이어와 Dense 레이어를 활용하는 예시는 아래와 같습니다. 즉, 활성화 함수 레이어를 사용하면 Dense 레이어에서 activation을 설정하지 않아도 됩니다. Dense layer와 Activation layer 사이에 BatchNormalization layer를 넣어줍니다.

```
tf.keras.layers.Dense(256),
tf.keras.layers.Activation('relu')
```
- 결과 : 이번 실습에서 사용한 데이터는 비교적 간단한 MNIST 데이터이고, 모델도 복잡하지 않기 때문에 배치 정규화를 적용한 결과와 그렇지 않은 결과의 차이가 크지 않을 수 있습니다. 그러나 MNIST보다 훨씬 고차원의 데이터를 더욱 복잡한 딥러닝 모델에 사용할 경우, 배치 정규화가 모델의 성능 향상에 큰 도움이 됩니다.

```
accuracy_basic:  0.10419604
accuracy_bn:  0.10319963
```

```
import numpy as np
import tensorflow as tf
from visual import *

import logging, os
logging.disable(logging.WARNING)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

# 배치 정규화를 적용할 모델과 비교하기 위한 기본 모델입니다.

def Basic():
    
    basic_model = tf.keras.Sequential([
                  tf.keras.layers.Flatten(input_shape=(28, 28)),
                  tf.keras.layers.Dense(256),
                  tf.keras.layers.Activation('relu'),
                  tf.keras.layers.Dense(128),
                  tf.keras.layers.Activation('relu'),
                  tf.keras.layers.Dense(512),
                  tf.keras.layers.Activation('relu'),
                  tf.keras.layers.Dense(64),
                  tf.keras.layers.Activation('relu'),
                  tf.keras.layers.Dense(128),
                  tf.keras.layers.Activation('relu'),
                  tf.keras.layers.Dense(256),
                  tf.keras.layers.Activation('relu'),
                  tf.keras.layers.Dense(10, activation='softmax')
                  ])
    
    return basic_model

'''
1. 기본 모델에 배치 정규화 레이어를 적용한 
   모델을 생성합니다. 입력층과 출력층은 그대로 사용합니다.
'''

def BN():
    
    bn_model = tf.keras.models.Sequential([
                  tf.keras.layers.Flatten(input_shape=(28, 28)),
                  tf.keras.layers.Dense(256),
                  tf.keras.layers.BatchNormalization(),
                  tf.keras.layers.Activation('relu'),
                  tf.keras.layers.Dense(128),
                  tf.keras.layers.BatchNormalization(),
                  tf.keras.layers.Activation('relu'),
                  tf.keras.layers.Dense(512),
                  tf.keras.layers.BatchNormalization(),
                  tf.keras.layers.Activation('relu'),
                  tf.keras.layers.Dense(64),
                  tf.keras.layers.BatchNormalization(),
                  tf.keras.layers.Activation('relu'),
                  tf.keras.layers.Dense(128),
                  tf.keras.layers.BatchNormalization(),
                  tf.keras.layers.Activation('relu'),
                  tf.keras.layers.Dense(256),
                  tf.keras.layers.BatchNormalization(),
                  tf.keras.layers.Activation('relu'),
                  tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    return bn_model

'''
2. 두 개의 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.

   Step01. Basic, BN 함수를 이용해 두 모델을 불러옵니다.
   
   Step02. 두 모델의 손실 함수, 최적화 알고리즘, 
           평가 방법을 설정합니다.
   
   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.
   
   Step04. 두 모델을 학습시킵니다. 
           두 모델 모두 'epochs'는 20,
           'batch_size'는 500으로 설정합니다. 
           검증용 데이터도 설정해주세요.
   
   Step05. 두 모델을 테스트하고 accuracy 값을 출력합니다. 
           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.
'''

def main():
    
    # MNIST 데이터를 불러오고 전처리합니다.
    mnist = tf.keras.datasets.mnist
    (train_data, train_labels), (test_data, test_labels) = mnist.load_data()
    train_data, test_data = train_data / 255.0, test_data / 255.0
    
    basic_model = Basic()  # 기본 모델입니다.
    bn_model = BN()     # 배치 정규화를 적용할 모델입니다.
    
    basic_model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'sparse_categorical_crossentropy'])
    bn_model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'sparse_categorical_crossentropy'])
    
    
    basic_history = basic_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, validation_data = (test_data, test_labels), verbose = 1)
    print('\n')
    bn_history = bn_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, validation_data = (test_data, test_labels), verbose = 1)
    
    scores_basic = basic_model.evaluate(test_data, test_labels)
    scores_bn = bn_model.evaluate(test_data, test_labels)
    
    print('\naccuracy_basic: ', scores_basic[-1])
    print('accuracy_bn: ', scores_bn[-1])
    
    Visulaize([('Basic', basic_history),('Batch Normalization', bn_history)])
    
    return basic_history, bn_history

if __name__ == "__main__":
    main()
```

### 정규화(Regularization)와 드롭아웃(Dropout)을 활용한 모델 성능 개선하기

L1, L2 정규화(Regularizaiton)와 드롭아웃(drop out)을 활용해 Fashion-MNIST 데이터를 분류하는 모델의 정확도를 86% 이상으로 향상시켜보도록 하겠습니다.

- 결과 :
```
accuracy_develop:  0.8796
```

```
import numpy as np
import tensorflow as tf
from visual import *
from plotter import *
from dataloader import load_data

import logging, os
logging.disable(logging.WARNING)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

np.random.seed(100)

'''
1. 입력층과 출력층은 그대로 사용합니다.
'''

def Develop():
    
    model = tf.keras.Sequential([
                  tf.keras.layers.Flatten(input_shape=(28, 28)),
                  tf.keras.layers.Dense(256, activation='relu', kernel_regularizer = tf.keras.regularizers.l1(0.001)),
                  tf.keras.layers.Dense(128, activation='relu', kernel_regularizer = tf.keras.regularizers.l2(0.001)),
                  tf.keras.layers.Dense(256, activation='relu'),
                  tf.keras.layers.Dense(64),
                  tf.keras.layers.BatchNormalization(),
                  tf.keras.layers.Activation('relu'),
                  tf.keras.layers.Dropout(0.2),
                  tf.keras.layers.Dense(10, activation='softmax')
                  ])
    
    return model
    
    
'''
2. 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.

   Step01. Develop 함수를 이용해 두 모델을 불러옵니다.
   
   Step02. 모델의 손실 함수, 최적화 알고리즘, 평가 방법을 설정합니다.
   
   Step03. 모델의 구조를 확인하는 코드를 작성합니다.
   
   Step04. 모델을 학습시킵니다. 두 모델 모두 'epochs'는 20,
           'batch_size'는 500으로 설정합니다. 검증용 데이터도 설정해주세요.
   
   Step05. 모델을 테스트하고 accuracy 점수를 출력합니다. 
           모델의 성능을 확인해보고, 목표값을 달성해보세요.
'''

def main():
    
    # Fashion-MNIST 데이터를 불러오고 전처리하는 부분입니다.
    (train_images, train_labels), (test_images, test_labels) = load_data()
    
    train_images = train_images / 255.0
    test_images = test_images / 255.0
    
    develop_model = Develop()
    
    develop_model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])

    history = develop_model.fit(train_images, train_labels, epochs = 40, batch_size = 1000, validation_data = (test_images, test_labels))
    
    scores = develop_model.evaluate(test_images, test_labels)
    
    print('\naccuracy_develop: ', scores[-1])
    
    Visulaize([('Develop', history)])
    
    return history

if __name__ == "__main__":
    main()
```
