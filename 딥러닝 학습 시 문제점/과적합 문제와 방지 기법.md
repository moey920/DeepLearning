과적합 문제와 방지 기법
05
05 과적합과 방지 기법
딥러닝 모델 학습에서의 과적합 방지 기법
다양한 과적합 현상 방지 기법 등장
정규화(Regularization)
드롭아웃(Dropout)
배치 정규화(Batch Normalization)
모델이 복잡해질수록
parameter들은 많아지고,
절댓값이 커지는 경향이 발생함
기존 손실함수에 규제항을 더해 최적값 찾기 가능
05 과적합 문제와 방지 기법
정규화(Regularization)
05 과적합 문제와 방지 기법
L1 정규화(Lasso Regularization)
가중치의 절댓값의 합을 규제항으로 정의
작은 가중치들이 거의 0으로 수렴하여 몇개의 중요한 가중치들만 남음
05 과적합 문제와 방지 기법
L2 정규화(Ridge Regularization)
가중치의 제곱의 합을 규제항으로 정의
L1 정규화에 비하여 0으로 수렴하는 가중치가 적음
큰 값을 가진 가중치를 더욱 제약하는 효과
05 과적합 문제와 방지 기법
드롭아웃(DropOut)
드롭아웃 적용
각 layer마다 일정 비율의 뉴런을 임의로 drop시켜 나머지 뉴런들만 학습하는 방법
드롭아웃을 적용하면 학습되는 노드와 가중치들이 매번 달라짐
05 과적합 문제와 방지 기법
드롭아웃의 적용 방법
다른 정규화 기법들과 상호 보완적으로 사용 가능
drop된 뉴런은 backpropagation 때 신호를 차단
Test 때는 모든 뉴런에 신호를 전달
05 과적합 문제와 방지 기법
배치 정규화(Batch Normalization)
∑
Fully connected
Batch Normalization
Activation function
Normalization을 처음 Input data 뿐만 아니라
신경망 내부 Hidden Layer의 input에도 적용
05 과적합 문제와 방지 기법
배치 정규화의 장점
매 Layer마다 정규화를 진행하므로
가중치 초기값에 크게 의존하지 않음(초기화 중요도 감소)
과적합 억제(Dropout, L1,L2 정규화 필요성 감소)
핵심은 학습 속도의 향상
