# 딥러닝 모델 학습의 문제점

- 데이터의 증가와 딥러닝 모델의 한계점
    - 실생활 문제 데이터의 차원이 증가하고, 구조가 복잡해짐
    - 다양한 문제가 발생하게 되고 기술력의 부족으로 딥러닝 기술은 실질적인 한계를 마주함

- 실질적인 문제
1. 학습 속도 문제
    - 데이터의 개수가 폭발적으로 증가하여 딥러닝 모델 학습 시 소요되는 시간도 함께 증가
2. 기울기 소실 문제
    - 더 깊고 더 넓은 망을 학습시키는 과정에서 출력값과 멀어질수록 학습이 잘 안되는 현상 발생
    - backpropagation을 활용하는데 미분해서 기울기를 찾는 과정에서 기울기가 소실됌(0으로 수렴한다)
    - 이전의 gradient를 계속 곱해가는데 0에 가까워지는 gradient를 곱해가기 때문에 발생한다.
3. 초기값 설정 문제
    - 초기값 설정 방식에 따른 성능 차이가 매우 크게 발생
    - 다양한 weight,bias 중 초기값을 설정해주어야 한다. 이 초기값에 따라 학습결과가 크게 달라진다.
4. 과적합 문제
    - 학습 데이터(training data)에 모델이 과하게 최적화되어 테스트 데이터(test data)에 대한 모델 성능이 저하
    - 특히 딥러닝은 파라미터가 많기(모델이 복잡하기) 때문에 학습데이터를 외워버리는 경우가 발생. 테스트 데이터에 대해서는 대응하지 못하는 상황이 발생한다. 

- 이러한 문제들로 인해 1990년대 두 번째 AI 빙하기를 맞이한다

# 학습 속도 문제와 최적화 알고리즘

- 학습 속도 문제의 발생 원인
    - 전체 학습 데이터 셋을 사용하여 손실 함수를 계산하기 때문에 계산량이 너무 많아짐 => batch 개념의 등장

- 학습 속도 문제 해결 방법
    - 전체 데이터가 아닌 **부분 데이터만 활용**하여 손실 함수를 계산하자
    - SGD(Stochastic Gradient Descent) : 무작위 Gradient Descent
        - 전체 데이터(batch) 대신 일부 조그마한 데이터의 모음인 **미니 배치(mini-batch)에 대해서만 손실 함수를 계산**
        - 빠른 시간에 더 많이 학습하는 SGD 알고리즘
        - 훨씬 계산 속도가 빠르기 때문에 같은 시간에 더 많은 step을 갈 수 있음(처음엔 정확도가 떨어질 수도 있으나 많은 계산을 통해 극복)
    - SGD의 한계
        - Gradient 방향성 문제 : SGD는 gradient 값 계산 시, mini-batch에 따라 gradient 방향의 변화가 큼
        - Learning Rate 설정 문제 : 너무 Learning Rate(알파) 크거나, 작으면 최적값에 도달하지 않았는데 수렴할 수도, 아예 이상한 값을 도출할 수도 있다. => 적합한 Learning Rate를 찾는 것도 중요하다!

- 다양한 최적화 알고리즘의 등장
    - GD(Gradient Descent) : 모든 데이터를 검토한 뒤 방향 찾기
    - SGD(Stochastic Gradient Descent) : Mini-batch를 사용하여 검토한 뒤 자주 방향 찾기
        - Learning Rate(알파)를 정해놓고(0.1, 0.3 등) 해보니 학습이 잘 안되거나, 시간이 오래걸리는 등의 문제가 발생할 수 있었음
        - 그래서 Adaptive Gradient가 등장했다.
        - Momentum(Gradient의 방향)
        - 관성의 개념을 도입하여 전에 이동했던 Gradient를 참고하여 업데이트
        - 과거에 이동했던 방식을 기억하면서 그 방향으로 일정 정도를 추가적으로 이동하는 방식
        - 산을 올라갔다, 내려갔다하면서 정상에 다다르기 어려운 것과 같다.
    - AdaGrad(Adaptive Gradient)
        - 가중치의 변화량에 따라 Learning rate 조절
        - 많이 변화하지 않은 변수들은 Learning rate를 크게 한다
        - 많이 변화했던 변수들은 Learning rate를 작게 한다(이미 많이 학습했다)
        - 과거의 기울기를 제곱해서 계속 더하기 때문에 학습이 진행될수록 갱신 강도가 약해짐
        - Learning Rate를 fix해놓고 학습시키는 것보단 효율적이였다.
    - RMSProp
        - AdaGrad의 학습을 진행할수록 줄어들 수 있는 Learning rate 문제를 해결
        - 무한히 학습하다보면 순간 갱신량이 0에 가까워 학습이 되지 않는 Adagrad의 단점을 해결
        - 과거의 기울기는 잊고 **새로운 기울기 정보**를 크게 반영(최근 기울기가 가장 큰 영향)
    - Adam(가장 발전된 최적화 알고리즘)
        - Momentum + RMSProp 장점을 결합
```
GD -> SGD ->(Gradient)->    Momentum              ->Adam
          ->(Learning Rate) -> Adagrad -> RMSProp -> /
```


### GD vs SGD(Stochastic Gradient Descent)

이번 실습에서는 동일한 모델 생성 및 학습을 통하여 두 최적화 기법을 비교해보도록 하겠습니다.

데이터셋은 IMDB 영화 리뷰 데이터셋을 사용합니다. 해당 데이터셋은 훈련용 데이터 25,000개와 테스트용 데이터 25,000개로 이루어져 있으며, 레이블은 긍정/부정으로 두 가지입니다. 이때 긍정은 1, 부정은 0으로 표시되어 있습니다. 우리의 목표는 전처리된 영화 리뷰 데이터를 가지고 그 리뷰가 긍정적인지 혹은 부정적인지를 예측하는 것입니다.

- 결과 : sgd가 손실이 더 낮다는 것을 확인할 수 있다
```
scores_gd:  0.7018452
scores_sgd:  0.6375151
```

```
import numpy as np
import tensorflow as tf
from visual import *

import logging, os
logging.disable(logging.WARNING)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

# 데이터를 전처리하는 함수
# 단어가 있는 위치를 1로 바꿈
def sequences_shaping(sequences, dimension):
    
    results = np.zeros((len(sequences), dimension))
    for i, word_indices in enumerate(sequences):
        results[i, word_indices] = 1.0 
        
    return results

'''
1. GD를 적용할 모델을 자유롭게 생성합니다.
'''

def GD_model(word_num):
    
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(32, input_shape = (word_num,), activation = 'relu'),
        tf.keras.layers.Dense(32, activation = 'relu'),
        tf.keras.layers.Dense(1, activation = 'sigmoid')
    ])
    
    return model
    
'''
2. SGD를 적용할 모델을 GD를 적용할 모델과 똑같이 생성합니다.
'''

def SGD_model(word_num):
    
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(32, input_shape = (word_num,), activation = 'relu'),
        tf.keras.layers.Dense(32, activation = 'relu'),
        tf.keras.layers.Dense(1, activation = 'sigmoid')
    ])
    
    return model

'''
3. 두 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.

   Step01. GD 함수와 SGD 함수를 이용해 
           두 모델을 불러옵니다.
   
   Step02. 두 모델의 손실 함수, 최적화 알고리즘, 
           평가 방법을 설정합니다.
   
   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.
   
   Step04. 두 모델을 각각 학습시킵니다. 
           검증용 데이터도 설정해주세요.
           'epochs'는 20으로 설정합니다.
   
           GD를 적용할 경우 학습 시 
           전체 데이터 셋(full-batch)을
           사용하므로 'batch_size'를 
           전체 데이터 개수로 설정합니다. 
           
           SGD를 적용할 경우 학습 시 
           미니 배치(mini-batch)를 사용하므로
           'batch_size'를 전체 데이터 개수보다 
           작은 수로 설정합니다. 
           
           여기선 500으로 설정하겠습니다.
   
   Step05. 학습된 두 모델을 테스트하고 
           binary crossentropy 값을 출력합니다. 
           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.
'''

def main():
    
    word_num = 100
    data_num = 25000
    
    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.
    
    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)
    
    train_data = sequences_shaping(train_data, dimension = word_num)
    test_data = sequences_shaping(test_data, dimension = word_num)
    
    gd_model = GD_model(word_num)   # GD를 사용할 모델입니다.
    sgd_model = SGD_model(word_num)  # SGD를 사용할 모델입니다.
    
    gd_model.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy', 'binary_crossentropy'])
    sgd_model.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy', 'binary_crossentropy'])
    
    gd_history = gd_model.fit(train_data, train_labels, epochs = 20, batch_size = data_num, validation_data = (test_data, test_labels), verbose = 1)
    print('\n')
    sgd_history = sgd_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, validation_data = (test_data, test_labels), verbose = 1)
    
    scores_gd = gd_history.history['val_binary_crossentropy'][-1]
    scores_sgd = sgd_history.history['val_binary_crossentropy'][-1]
    
    print('\nscores_gd: ', scores_gd)
    print('scores_sgd: ', scores_sgd)
    
    Visulaize([('GD', gd_history),('SGD', sgd_history)])
    
    return gd_history, sgd_history

if __name__ == "__main__":
    main()
```

### 모멘텀

SGD에서 momentum을 사용하기 위한 함수/라이브러리

- `tf.keras.optimizers.SGD(lr, momentum)`
    - lr : 학습률 (learning rate) (lr >= 0), 기본값 0.1
    - momentum : 진동을 막아주고 SGD를 가속하는 파라미터 (momentum >= 0), 기본값 0.9

- 모델을 테스트하기 위한 함수/라이브러리
    - `score = model.evaluate(test_data, test_labels)` : 테스트 데이터와 테스트 label에 대한 모델의 성능을 나타냅니다. compile 과정에서 어떤 평가 방법(metrics)을 쓰느냐에 따라 다양한 성능 지표가 나옵니다.

실습을 통해서 모멘텀 기법을 활용해 봅시다. 이번 실습에서는 이전 실습과 동일한 IMDB 영화 리뷰 데이터셋을 활용합니다. 모멘텀을 사용한 결과가 더 좋은 것을 알 수 있습니다.

```
import numpy as np
import tensorflow as tf
from visual import *

import logging, os
logging.disable(logging.WARNING)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

# 데이터를 전처리하는 함수

def sequences_shaping(sequences, dimension):
    
    results = np.zeros((len(sequences), dimension))
    for i, word_indices in enumerate(sequences):
        results[i, word_indices] = 1.0 
    
    return results
    
'''
1. 모멘텀(momentum)을 적용/비적용 할 하나의 모델을 자유롭게 생성합니다.
'''
    
def Momentum_model(word_num):
    
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(32, input_shape = (word_num,), activation = 'relu'),
        tf.keras.layers.Dense(32, activation = 'relu'),
        tf.keras.layers.Dense(1, activation = 'sigmoid')
    ])
    
    return model

'''
2. 두 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.

   Step01. Momentum_model 함수를 이용해 
           두 모델을 불러옵니다. 모두 동일한 모델입니다.
   
   Step02. 두 모델의 손실 함수, 최적화 알고리즘, 
           평가 방법을 설정합니다.
   
   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.
   
   Step04. 두 모델을 각각 학습시킵니다. 
           검증용 데이터도 설정해주세요.
           두 모델 모두 'epochs'는 20, 'batch_size'는
           500으로 설정합니다.
   
   Step05. 학습된 두 모델을 테스트하고 
           binary crossentropy 값을 출력합니다. 
           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.
'''
    
def main():
    
    word_num = 100
    data_num = 25000
    
    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.
    
    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)
    
    train_data = sequences_shaping(train_data, dimension = word_num)
    test_data = sequences_shaping(test_data, dimension = word_num)
    
    sgd_model = Momentum_model(word_num)   # 모멘텀을 사용하지 않을 모델입니다.
    msgd_model = Momentum_model(word_num)  # 모멘텀을 사용할 모델입니다.
    
    sgd_opt = tf.keras.optimizers.SGD(lr = 0.01, momentum = 0)
    sgd_model.compile(loss = 'binary_crossentropy', optimizer = sgd_opt, metrics = ['accuracy', 'binary_crossentropy'])
    
    msgd_opt = tf.keras.optimizers.SGD(lr = 0.01, momentum = 0.7)
    msgd_model.compile(loss = 'binary_crossentropy', optimizer = msgd_opt, metrics = ['accuracy', 'binary_crossentropy'])
    
    sgd_model.summary()
    msgd_model.summary()
    
    sgd_history = sgd_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, validation_data = (test_data, test_labels), verbose = 1)
    print('\n')
    msgd_history = msgd_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, validation_data = (test_data, test_labels), verbose = 1)
    
    scores_sgd = sgd_model.evaluate(test_data, test_labels)
    scores_msgd = msgd_model.evaluate(test_data, test_labels)
    
    print('\nscores_sgd: ', scores_sgd[-1])
    print('scores_msgd: ', scores_msgd[-1])
    
    Visulaize([('SGD', sgd_history),('mSGD', msgd_history)])
    
    return sgd_history, msgd_history

if __name__ == "__main__":
    main()
```

### Adagrad, RMSprop, Adam 최적화(optimize) 알고리즘

**Adagrad**  

Adagrad(Adaptive Gradient) 최적화 알고리즘은 손실 함수(loss function)의 값을 최소로 만드는 최적의 가중치를 찾아내기 위해 learning rate를 조절해 하강하는 방법 중 하나입니다.

**기존 방식이 가중치들의 업데이트를 같은 속도로 한꺼번에 하는 방법**이었다면, Adagrad는 **가중치 각각의 업데이트 속도를 데이터에 맞추어(adaptively) 계산해 적절한 learning rate로 하강**하도록 합니다.

- `tf.keras.optimizers.Adagrad(lr, epsilon, decay)`
    - lr : 학습률 (learning rate) (lr >= 0), 기본값 0.1
    - epsilon : 연산 시 분모가 0이 되는 것을 막는, 0에 가까운 상수 (epsilon >= 0), 기본값 0.000001
    - decay : 업데이트마다 학습률을 비율만큼 줄여주는 파라미터 (decay >= 0), 기본값 0.0

**RMSprop**

RMSprop 최적화 알고리즘은 학습이 진행될수록 가중치 업데이트 강도가 약해지는 Adagrad의 단점을 보완하고자 제안된 방법입니다.

RMSProp은 **과거의 gradient 값은 잊고 새로운 gradient 값을 크게 반영**해서 가중치를 업데이트합니다.

- `tf.keras.optimizers.RMSprop(lr)`
    - lr : 학습률, 기본값 0.1

**Adam**

Adam은 최적화 알고리즘 중 가장 발전된 기법입니다. **RMSProp과 모멘텀(momentum)을 함께 사용**함으로써, 진행 방향과 learning rate 모두를 적절하게 유지하면서 학습할 수 있도록 고안되었습니다.

- `tf.keras.optimizers.Adam(lr, beta_1, beta_2)`
    - lr : 학습률, 기본값 0.01
    - beta_1 : 모멘텀을 결정하기 위해 사용하는 파라미터 (beta_1 >= 0 ), 기본값 0.9
    - beta_2 : step size를 결정하기 위해 사용하는 파라미터 (beta_2 >= 0), 기본값 0.999

실습을 통해서 각각의 최적화 알고리즘을 활용해 봅시다. 이번 실습에서는 이전 실습과 동일한 IMDB 영화 리뷰 데이터셋을 활용합니다.

일반적으로 Adam의 성능이 제일 좋고, 그다음 RMSProp, Adagrad 순으로 성능이 좋다고 알려져 있습니다.

그러나 데이터셋의 종류, epochs나 batch_size 같은 하이퍼 파라미터의 값 등등에 따라 최적화 알고리즘의 성능은 이론과 완전히 같지 않을 수 있습니다.

- 결과
```
scores_adagrad:  0.69328445
scores_rmsprop:  0.5673302
scores_adam:  0.5834081
```

```
import numpy as np
import tensorflow as tf
from visual import *

import logging, os
logging.disable(logging.WARNING)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

# 데이터를 전처리하는 함수

def sequences_shaping(sequences, dimension):
    
    results = np.zeros((len(sequences), dimension))
    for i, word_indices in enumerate(sequences):
        results[i, word_indices] = 1.0 
    
    return results

'''
1. Adagrad, RMSprop, Adam 최적화 알고리즘을 적용할 하나의 모델을 자유롭게 생성합니다.
'''

def OPT_model(word_num):
    
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(32, input_shape = (word_num,), activation = 'relu'),
        tf.keras.layers.Dense(32, activation = 'relu'),
        tf.keras.layers.Dense(1, activation = 'sigmoid')
    ])
    
    return model

'''
2. 세 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.

   Step01. OPT_model 함수를 이용해 세 모델을 불러옵니다. 
           모두 동일한 모델입니다.
   
   Step02. 세 모델의 손실 함수, 최적화 방법, 
           평가 방법을 설정합니다.
   
   Step03. 세 모델의 구조를 확인하는 코드를 작성합니다.
   
   Step04. 세 모델을 각각 학습시킵니다. 
           세 모델 모두 'epochs'는 20, 'batch_size'는
           500으로 설정합니다.
   
   Step05. 세 모델을 테스트하고 
           binary crossentropy 점수를 출력합니다. 
           셋 중 어느 모델의 성능이 가장 좋은지 확인해보세요.
'''

def main():
    
    word_num = 100
    data_num = 25000
    
    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.
    
    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)
    
    train_data = sequences_shaping(train_data, dimension = word_num)
    test_data = sequences_shaping(test_data, dimension = word_num)
    
    adagrad_model = OPT_model(word_num)  # Adagrad를 사용할 모델입니다.
    rmsprop_model = OPT_model(word_num)  # RMSProp을 사용할 모델입니다.
    adam_model = OPT_model(word_num)     # Adam을 사용할 모델입니다.
    
    adagrad_opt = tf.keras.optimizers.Adagrad(lr = 0.01, epsilon = 0.00001, decay = 0.4)
    adagrad_model.compile(loss = 'binary_crossentropy', optimizer = adagrad_opt, metrics =  ['accuracy', 'binary_crossentropy'])
    
    rmsprop_opt = tf.keras.optimizers.RMSprop(lr = 0.001)
    rmsprop_model.compile(loss = 'binary_crossentropy', optimizer = rmsprop_opt, metrics =  ['accuracy', 'binary_crossentropy'])
    
    adam_opt = tf.keras.optimizers.Adam(lr = 0.01, beta_1 = 0.9, beta_2 = 0.999)
    adam_model.compile(loss = 'binary_crossentropy', optimizer = adam_opt, metrics =  ['accuracy', 'binary_crossentropy'])
    
    
    adagrad_history = adagrad_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, validation_data = (test_data, test_labels), verbose = 0)
    print('\n')
    rmsprop_history = rmsprop_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, validation_data = (test_data, test_labels), verbose = 0)
    print('\n')
    adam_history = adam_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, validation_data = (test_data, test_labels), verbose = 0)
    
    scores_adagrad = adagrad_model.evaluate(test_data, test_labels, verbose = 1)
    scores_rmsprop = rmsprop_model.evaluate(test_data, test_labels, verbose = 1)
    scores_adam = adam_model.evaluate(test_data, test_labels, verbose = 1)
    
    print('\nscores_adagrad: ', scores_adagrad[-1])
    print('scores_rmsprop: ', scores_rmsprop[-1])
    print('scores_adam: ', scores_adam[-1])
    
    Visulaize([('Adagrad', adagrad_history),('RMSprop', rmsprop_history),('Adam', adam_history)])
    
    return adagrad_history, rmsprop_history, adam_history
    
if __name__ == "__main__":
    main()
```
