# ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í•™ìŠµ ë°©ë²•

- ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ë€ ? Hidden Layerê°€ 3ì¸µ ì´ìƒì¼ ì‹œ ê¹Šì€ ì‹ ê²½ë§ì´ë¼ëŠ” ì˜ë¯¸ì˜ Deep Learning ë‹¨ì–´ ì‚¬ìš©
- ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ êµ¬ì„± ìš”ì†Œ   
    - ê° ì¸µì„ êµ¬ì„±í•˜ëŠ” ìš”ì†Œ : Nodes(Units) 
    - ë…¸ë“œê°„ì˜ ì—°ê²°ê°•ë„ : Weight(ê°€ì¤‘ì¹˜)
    - ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ” ì¸µ : Layer
- ë”¥ëŸ¬ë‹ì—ì„œì˜ í•™ìŠµì´ë€ : Loss functionì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©

## ì†ì‹¤ í•¨ìˆ˜(Loss Function)ê³¼ ìµœì í™”(Optimization)
- ê°€ì¥ ì í•©í•œ ê°€ì¤‘ì¹˜(w) ê°’ì„ ì°¾ëŠ” ê²ƒ
- Loss Function : ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ê°„ì˜ ì˜¤ì°¨ê°’
- Optimization : ì˜¤ì°¨ê°’ì„ ìµœì†Œí™”í•˜ëŠ” ëª¨ë¸ì˜ ì¸ìë¥¼ ì°¾ëŠ” ê²ƒ
- ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë°©ë²• : ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°„ì˜ ì˜¤ì°¨ê°’ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ì˜¤ì°¨ê°’ì„ ìµœì†Œí™”í•˜ëŠ” ëª¨ë¸ì˜ ì¸ìë¥¼ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©

## ê°€ì¥ ê¸°ë³¸ì ì¸ ìµœì í™” ì•Œê³ ë¦¬ì¦˜, Gradient Descent(GD)

`ğ‘Šğ‘¡+1 = ğ‘Šğ‘¡ âˆ’ ğ›¼âˆ‡ğ¿ğ‘œğ‘ ğ‘ (ğ‘Š)`  
`Gradient(ğ›ğ‹ğ¨ğ¬ğ¬(ğ–))` = íŠ¹ì • ê°€ì¤‘ì¹˜ì—ì„œì˜ ê¸°ìš¸ê¸°

> ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ë“¤ì„ ğ‘Šë¼ê³  í–ˆì„ ë•Œ, ì†ì‹¤í•¨ìˆ˜ ğ¿ğ‘œğ‘ ğ‘ (ğ‘Š)ì˜ ê°’ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ê¸°ìš¸ê¸° âˆ‡ğ¿ğ‘œğ‘ ğ‘ (ğ‘Š) ë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•

- ê° ê°€ì¤‘ì¹˜ë“¤ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” ë°©ë²• : ë”¥ëŸ¬ë‹ì—ì„œëŠ” ì—­ì „íŒŒ(Backpropagation)ì„ í†µí•´ ê° ê°€ì¤‘ì¹˜ë“¤ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ
    - Boom timesì˜ ë°°ê²½ : ì—­ì „íŒŒ(Backpropogation) - 1980ë…„ëŒ€ AI ë¶€í¥ì„ ì´ë”, ì—­ì „íŒŒë¥¼ ì´ìš©í•´ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œê²Œ ëŒ

## ì—­ì „íŒŒ(Backpropogation)ì˜ ì •ì˜

> ë‚˜ì˜ ëª©í‘œ target ê°’ê³¼ ì‹¤ì œ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ output ê°’ì´ ì–¼ë§ˆë‚˜ ì°¨ì´ë‚˜ëŠ”ì§€ êµ¬í•œ í›„ ì˜¤ì°¨ê°’ì„ ë‹¤ì‹œ ë’¤ë¡œ ì „íŒŒí•´ê°€ë©° ë³€ìˆ˜ë“¤ì„ ê°±ì‹ í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜

- ìˆœì „íŒŒ(Forward propagation) ì •ì˜ : ì…ë ¥ ê°’ì„ ë°”íƒ•ìœ¼ë¡œ ì¶œë ¥ ê°’ì„ ê³„ì‚°í•˜ëŠ” ê³¼ì •
- ì—­ì „íŒŒ(Backpropagation) : ì´ì „ Layerì˜ ë³€ìˆ˜ë“¤ì´ ì–´ë–»ê²Œ ë˜ì–´ì•¼ ì •í™•í•œ target ê°’ì„ êµ¬í•´ë‚¼ ìˆ˜ ìˆì„ê¹Œ! Forward propagationì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì´ë£¨ì–´ì§€ëŠ” ê³¼ì •

### Gradient descent ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„í•˜ê¸°

##### Gradient descent

Gradient descent ì•Œê³ ë¦¬ì¦˜ì€ **ì†ì‹¤ í•¨ìˆ˜(loss function)ì˜ ë¯¸ë¶„ê°’ì¸ gradientë¥¼ ì´ìš©**í•´ ëª¨ë¸ì—ê²Œ ë§ëŠ” **ìµœì ì˜ ê°€ì¤‘ì¹˜(weight)**, ì¦‰ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ìµœì†Œí™”í•˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•  ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” Gradient descent ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ êµ¬í˜„í•œ í›„, ì´ë¥¼ ì´ìš©í•´ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” **ì„ í˜• íšŒê·€ ì§ì„ ì˜ ê¸°ìš¸ê¸°ì™€ yì ˆí¸**, ì¦‰ ì„ í˜• íšŒê·€ ëª¨ë¸ì—ê²Œ ë§ëŠ” **ìµœì ì˜ ê°€ì¤‘ì¹˜**ë¥¼ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤.

ì„ í˜• íšŒê·€ ì§ì„ ì˜ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ì€ 1ì°¨ í•¨ìˆ˜ í˜•íƒœì´ë©°, ìš°ë¦¬ê°€ Gradient descent ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ ì°¾ì„ ê°’, ì¦‰ ê°€ì¤‘ì¹˜ëŠ” w0, w1ì…ë‹ˆë‹¤.  
`f(x) = w0 + w1x`

##### ì†ì‹¤ í•¨ìˆ˜ (loss function)
ì†ì‹¤ í•¨ìˆ˜(loss function)ëŠ” **ì‹¤ì œê°’ê³¼ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’ ê°„ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•´ì£¼ëŠ” í•¨ìˆ˜**ì…ë‹ˆë‹¤. **ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì€ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°ì— ì‚¬ìš©**ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì†ì‹¤ í•¨ìˆ˜ë¡œ MSE (Mean Squared Error)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

**MSEëŠ” í‰ê·  ì œê³± ì˜¤ì°¨ í•¨ìˆ˜**ì…ë‹ˆë‹¤.

##### í¸ë¯¸ë¶„

gradientì—ëŠ” **í¸ë¯¸ë¶„**ì´ë¼ëŠ” ê°œë…ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤. ë”°ë¼ì„œ gradientë¥¼ ì„¤ëª…í•˜ê¸° ì „ í¸ë¯¸ë¶„ì— ëŒ€í•´ ê°„ë‹¨í•˜ê²Œ ì§šê³  ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤. 

> í¸ë¯¸ë¶„ì´ë€ 2ê°œ ì´ìƒì˜ ë³€ìˆ˜ë¥¼ ê°€ì§„ í•¨ìˆ˜ì—ì„œ ìš°ë¦¬ê°€ ë¯¸ë¶„í•  í•˜ë‚˜ì˜ ë³€ìˆ˜ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë“¤ì„ ìƒìˆ˜ë¡œ ë³´ê³ , ë¯¸ë¶„ ëŒ€ìƒì¸ ê·¸ ë³€ìˆ˜ë¡œ ë¯¸ë¶„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ `f(x,y) = 2x^2+y`ë¼ëŠ” ìˆ˜ì‹ì´ ìˆì„ ë•Œ, xì— ëŒ€í•´ì„œë§Œ í¸ë¯¸ë¶„í•œë‹¤ë©´ `fâ€˜(x,y) = 4x`ê°€ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

##### Gradient

gradientëŠ” ê³§ **ê¸°ìš¸ê¸° ë²¡í„°**ë¥¼ ì˜ë¯¸í•˜ë©°, **ì„ í˜• í•¨ìˆ˜ì˜ ê° íŒŒë¼ë¯¸í„°ì˜ í¸ë¯¸ë¶„ìœ¼ë¡œ êµ¬ì„±ëœ ì—´ë²¡í„°ë¡œ ì •ì˜**í•©ë‹ˆë‹¤.

í•™ìŠµë¥ (learning rate)ì„ ë‚˜íƒ€ë‚´ëŠ” Î±ê°€ ìˆê³ , gradientë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜ì‹ì¸ â–½Loss(W)ê°€ ìˆìŠµë‹ˆë‹¤. ì¦‰ ì´ë¥¼ í’€ì–´ì„œ ì“°ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì—´ë²¡í„° í˜•íƒœì…ë‹ˆë‹¤.

`gradient = â–½Loss(W) = [âˆ‚Loss/âˆ‚w0, âˆ‚Loss/âˆ‚w1]`

w0, w1ì— ëŒ€í•œ gradientë¥¼ êµ¬í•˜ê¸° ìœ„í•´ LossLossë¥¼ ê°ê°ì— ëŒ€í•´ í¸ë¯¸ë¶„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  
`âˆ‚Loss/âˆ‚w0 = 2/Nâˆ‘(yi-(w0+w1xi))(-1)`  
`âˆ‚Loss/âˆ‚w1 = 2/Nâˆ‘(yi-(w0+w1xi))(-xi)`  

â€‹	
 
##### ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
ìœ„ì™€ ê°™ì´ êµ¬í•œ w0ì™€ w1ì˜ greadientì™€ í•™ìŠµë¥  Î±ë¥¼ ì´ìš©í•´ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³µì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

`w0^t+1 = w0^t - Î±âˆ‚Loss/âˆ‚w0`  
`w1^t+1 = w1^t - Î±âˆ‚Loss/âˆ‚w1`  

```
import numpy as np

# ì‚¬ìš©í•  1ì°¨ ì„ í˜• íšŒê·€ ëª¨ë¸, radient descent ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ ì°¾ì„ ê°’, ì¦‰ ê°€ì¤‘ì¹˜ëŠ” w0, w1ì…ë‹ˆë‹¤.

def linear_model(w0, w1, X):
    
    f_x = w0 + w1 * X
    
    return f_x
    
'''
1. ì„¤ëª… ì¤‘ 'ì†ì‹¤ í•¨ìˆ˜' íŒŒíŠ¸ì˜ ìˆ˜ì‹ì„ ì°¸ê³ í•´
   MSE ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”. 
'''
# f_x : ì˜ˆì¸¡ê°’, y : ì •ë‹µê°’

def Loss(f_x, y):
    # ì •ë‹µê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ì˜ ì œê³±ì„ í‰ê· 
    ls = np.mean(np.square(y - f_x))
    
    return ls

'''
2. ì„¤ëª… ì¤‘ 'Gradient' íŒŒíŠ¸ì˜ ë§ˆì§€ë§‰ ë‘ ìˆ˜ì‹ì„ ì°¸ê³ í•´ ë‘ ê°€ì¤‘ì¹˜
   w0ì™€ w1ì— ëŒ€í•œ gradientì¸ 'gradient0'ì™€ 'gradient1'ì„
   ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ gradient_descent í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.
   
   Step01. w0ì— ëŒ€í•œ gradientì¸ 'gradient0'ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
   
   Step02. w1ì— ëŒ€í•œ gradientì¸ 'gradient1'ì„ ì‘ì„±í•©ë‹ˆë‹¤.
'''

# ê°€ì¤‘ì¹˜ w, ì…ë ¥ X, ì •ë‹µ y

def gradient_descent(w0, w1, X, y):
    # ì•ì„œ ê³„ì‚°í•œ loss functionì„ ê° weightìœ¼ë¡œ í¸ë¯¸ë¶„ì„ í•˜ë©´ gradient ê°’ì´ ëœë‹¤.
    gradient0 = 2 * np.mean((y - (w0 + w1 * X)) * (-1))
    gradient1 = 2 * np.mean((y - (w0 + w1 * X)) * (-1 * X))
    
    return np.array([gradient0, gradient1])

'''
3. ì„¤ëª… ì¤‘ 'ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸' íŒŒíŠ¸ì˜ ë‘ ìˆ˜ì‹ì„ ì°¸ê³ í•´ 
   gradient descentë¥¼ í†µí•œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
   
   Step01. ì•ì„œ ì™„ì„±í•œ gradient_descent í•¨ìˆ˜ë¥¼ ì´ìš©í•´
           w0ì™€ w1ì— ëŒ€í•œ gradientì¸ 'gd'ë¥¼ ì •ì˜í•˜ì„¸ìš”.
           
   Step02. ë³€ìˆ˜ 'w0'ì™€ 'w1'ì— ë‘ ê°€ì¤‘ì¹˜ w0ì™€ w1ì„ 
           ì—…ë°ì´íŠ¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤. ì•ì„œ ì •ì˜í•œ
           ë³€ìˆ˜ 'gd'ì™€ ì´ë¯¸ ì •ì˜ëœ ë³€ìˆ˜ 'lr'ì„ ì‚¬ìš©í•˜ì„¸ìš”.
'''

def main():
    
    X = np.array([1,2,3,4]).reshape((-1,1))
    y = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))
    
    # íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
    w0 = 0
    w1 = 0
    
    # learning rate ì„¤ì •
    lr = 0.001
    
    # ë°˜ë³µ íšŸìˆ˜ 1000ìœ¼ë¡œ ì„¤ì •
    for i in range(1000):
    
        gd = gradient_descent(w0, w1, X, y)
        
        w0 = w0 - lr * gd[0]
        w1 = w1 - lr * gd[1]
        
        # 100íšŒë§ˆë‹¤ì˜ í•´ë‹¹ lossì™€ w0, w1 ì¶œë ¥
        if (i % 100 == 0):
        
            loss = Loss(linear_model(w0,w1,X),y)
        
            print("{}ë²ˆì§¸ loss : {}".format(i, loss))
            print("{}ë²ˆì§¸ w0, w1 : {}, {}".format(i, w0, w1),'\n')

    return w0, w1

if __name__ == '__main__':
    main()
```
