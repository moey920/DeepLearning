# ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í•™ìŠµ ë°©ë²•

- ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ë€ ? Hidden Layerê°€ 3ì¸µ ì´ìƒì¼ ì‹œ ê¹Šì€ ì‹ ê²½ë§ì´ë¼ëŠ” ì˜ë¯¸ì˜ Deep Learning ë‹¨ì–´ ì‚¬ìš©
- ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ êµ¬ì„± ìš”ì†Œ   
    - ê° ì¸µì„ êµ¬ì„±í•˜ëŠ” ìš”ì†Œ : Nodes(Units) 
    - ë…¸ë“œê°„ì˜ ì—°ê²°ê°•ë„ : Weight(ê°€ì¤‘ì¹˜)
    - ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ” ì¸µ : Layer
- ë”¥ëŸ¬ë‹ì—ì„œì˜ í•™ìŠµì´ë€ : Loss functionì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©

## ì†ì‹¤ í•¨ìˆ˜(Loss Function)ê³¼ ìµœì í™”(Optimization)
- ê°€ì¥ ì í•©í•œ ê°€ì¤‘ì¹˜(w) ê°’ì„ ì°¾ëŠ” ê²ƒ
- Loss Function : ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ê°„ì˜ ì˜¤ì°¨ê°’
- Optimization : ì˜¤ì°¨ê°’ì„ ìµœì†Œí™”í•˜ëŠ” ëª¨ë¸ì˜ ì¸ìë¥¼ ì°¾ëŠ” ê²ƒ
- ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë°©ë²• : ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°„ì˜ ì˜¤ì°¨ê°’ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ì˜¤ì°¨ê°’ì„ ìµœì†Œí™”í•˜ëŠ” ëª¨ë¸ì˜ ì¸ìë¥¼ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©

## ê°€ì¥ ê¸°ë³¸ì ì¸ ìµœì í™” ì•Œê³ ë¦¬ì¦˜, Gradient Descent(GD)

`ğ‘Šğ‘¡+1 = ğ‘Šğ‘¡ âˆ’ ğ›¼âˆ‡ğ¿ğ‘œğ‘ ğ‘ (ğ‘Š)`  
`Gradient(ğ›ğ‹ğ¨ğ¬ğ¬(ğ–))` = íŠ¹ì • ê°€ì¤‘ì¹˜ì—ì„œì˜ ê¸°ìš¸ê¸°

> ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ë“¤ì„ ğ‘Šë¼ê³  í–ˆì„ ë•Œ, ì†ì‹¤í•¨ìˆ˜ ğ¿ğ‘œğ‘ ğ‘ (ğ‘Š)ì˜ ê°’ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ê¸°ìš¸ê¸° âˆ‡ğ¿ğ‘œğ‘ ğ‘ (ğ‘Š) ë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•

- ê° ê°€ì¤‘ì¹˜ë“¤ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” ë°©ë²• : ë”¥ëŸ¬ë‹ì—ì„œëŠ” ì—­ì „íŒŒ(Backpropagation)ì„ í†µí•´ ê° ê°€ì¤‘ì¹˜ë“¤ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ
    - Boom timesì˜ ë°°ê²½ : ì—­ì „íŒŒ(Backpropogation) - 1980ë…„ëŒ€ AI ë¶€í¥ì„ ì´ë”, ì—­ì „íŒŒë¥¼ ì´ìš©í•´ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œê²Œ ëŒ

## ì—­ì „íŒŒ(Backpropogation)ì˜ ì •ì˜

> ë‚˜ì˜ ëª©í‘œ target ê°’ê³¼ ì‹¤ì œ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ output ê°’ì´ ì–¼ë§ˆë‚˜ ì°¨ì´ë‚˜ëŠ”ì§€ êµ¬í•œ í›„ ì˜¤ì°¨ê°’ì„ ë‹¤ì‹œ ë’¤ë¡œ ì „íŒŒí•´ê°€ë©° ë³€ìˆ˜ë“¤ì„ ê°±ì‹ í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜

- ìˆœì „íŒŒ(Forward propagation) ì •ì˜ : ì…ë ¥ ê°’ì„ ë°”íƒ•ìœ¼ë¡œ ì¶œë ¥ ê°’ì„ ê³„ì‚°í•˜ëŠ” ê³¼ì •
- ì—­ì „íŒŒ(Backpropagation) : ì´ì „ Layerì˜ ë³€ìˆ˜ë“¤ì´ ì–´ë–»ê²Œ ë˜ì–´ì•¼ ì •í™•í•œ target ê°’ì„ êµ¬í•´ë‚¼ ìˆ˜ ìˆì„ê¹Œ! Forward propagationì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì´ë£¨ì–´ì§€ëŠ” ê³¼ì •

### Gradient descent ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„í•˜ê¸°

##### Gradient descent

Gradient descent ì•Œê³ ë¦¬ì¦˜ì€ **ì†ì‹¤ í•¨ìˆ˜(loss function)ì˜ ë¯¸ë¶„ê°’ì¸ gradientë¥¼ ì´ìš©**í•´ ëª¨ë¸ì—ê²Œ ë§ëŠ” **ìµœì ì˜ ê°€ì¤‘ì¹˜(weight)**, ì¦‰ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ìµœì†Œí™”í•˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•  ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” Gradient descent ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ êµ¬í˜„í•œ í›„, ì´ë¥¼ ì´ìš©í•´ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” **ì„ í˜• íšŒê·€ ì§ì„ ì˜ ê¸°ìš¸ê¸°ì™€ yì ˆí¸**, ì¦‰ ì„ í˜• íšŒê·€ ëª¨ë¸ì—ê²Œ ë§ëŠ” **ìµœì ì˜ ê°€ì¤‘ì¹˜**ë¥¼ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤.

ì„ í˜• íšŒê·€ ì§ì„ ì˜ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ì€ 1ì°¨ í•¨ìˆ˜ í˜•íƒœì´ë©°, ìš°ë¦¬ê°€ Gradient descent ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ ì°¾ì„ ê°’, ì¦‰ ê°€ì¤‘ì¹˜ëŠ” w0, w1ì…ë‹ˆë‹¤.  
`f(x) = w0 + w1x`

##### ì†ì‹¤ í•¨ìˆ˜ (loss function)
ì†ì‹¤ í•¨ìˆ˜(loss function)ëŠ” **ì‹¤ì œê°’ê³¼ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’ ê°„ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•´ì£¼ëŠ” í•¨ìˆ˜**ì…ë‹ˆë‹¤. **ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì€ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°ì— ì‚¬ìš©**ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì†ì‹¤ í•¨ìˆ˜ë¡œ MSE (Mean Squared Error)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

**MSEëŠ” í‰ê·  ì œê³± ì˜¤ì°¨ í•¨ìˆ˜**ì…ë‹ˆë‹¤.

##### í¸ë¯¸ë¶„

gradientì—ëŠ” **í¸ë¯¸ë¶„**ì´ë¼ëŠ” ê°œë…ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤. ë”°ë¼ì„œ gradientë¥¼ ì„¤ëª…í•˜ê¸° ì „ í¸ë¯¸ë¶„ì— ëŒ€í•´ ê°„ë‹¨í•˜ê²Œ ì§šê³  ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤. 

> í¸ë¯¸ë¶„ì´ë€ 2ê°œ ì´ìƒì˜ ë³€ìˆ˜ë¥¼ ê°€ì§„ í•¨ìˆ˜ì—ì„œ ìš°ë¦¬ê°€ ë¯¸ë¶„í•  í•˜ë‚˜ì˜ ë³€ìˆ˜ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë“¤ì„ ìƒìˆ˜ë¡œ ë³´ê³ , ë¯¸ë¶„ ëŒ€ìƒì¸ ê·¸ ë³€ìˆ˜ë¡œ ë¯¸ë¶„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ `f(x,y) = 2x^2+y`ë¼ëŠ” ìˆ˜ì‹ì´ ìˆì„ ë•Œ, xì— ëŒ€í•´ì„œë§Œ í¸ë¯¸ë¶„í•œë‹¤ë©´ `fâ€˜(x,y) = 4x`ê°€ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

##### Gradient

gradientëŠ” ê³§ **ê¸°ìš¸ê¸° ë²¡í„°**ë¥¼ ì˜ë¯¸í•˜ë©°, **ì„ í˜• í•¨ìˆ˜ì˜ ê° íŒŒë¼ë¯¸í„°ì˜ í¸ë¯¸ë¶„ìœ¼ë¡œ êµ¬ì„±ëœ ì—´ë²¡í„°ë¡œ ì •ì˜**í•©ë‹ˆë‹¤.

í•™ìŠµë¥ (learning rate)ì„ ë‚˜íƒ€ë‚´ëŠ” Î±ê°€ ìˆê³ , gradientë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜ì‹ì¸ â–½Loss(W)ê°€ ìˆìŠµë‹ˆë‹¤. ì¦‰ ì´ë¥¼ í’€ì–´ì„œ ì“°ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì—´ë²¡í„° í˜•íƒœì…ë‹ˆë‹¤.

`gradient = â–½Loss(W) = [âˆ‚Loss/âˆ‚w0, âˆ‚Loss/âˆ‚w1]`

w0, w1ì— ëŒ€í•œ gradientë¥¼ êµ¬í•˜ê¸° ìœ„í•´ LossLossë¥¼ ê°ê°ì— ëŒ€í•´ í¸ë¯¸ë¶„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  
`âˆ‚Loss/âˆ‚w0 = 2/Nâˆ‘(yi-(w0+w1xi))(-1)`  
`âˆ‚Loss/âˆ‚w1 = 2/Nâˆ‘(yi-(w0+w1xi))(-xi)`  

â€‹	
 
##### ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
ìœ„ì™€ ê°™ì´ êµ¬í•œ w0ì™€ w1ì˜ greadientì™€ í•™ìŠµë¥  Î±ë¥¼ ì´ìš©í•´ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³µì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

`w0^t+1 = w0^t - Î±âˆ‚Loss/âˆ‚w0`  
`w1^t+1 = w1^t - Î±âˆ‚Loss/âˆ‚w1`  

```
import numpy as np

# ì‚¬ìš©í•  1ì°¨ ì„ í˜• íšŒê·€ ëª¨ë¸, radient descent ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ ì°¾ì„ ê°’, ì¦‰ ê°€ì¤‘ì¹˜ëŠ” w0, w1ì…ë‹ˆë‹¤.

def linear_model(w0, w1, X):
    
    f_x = w0 + w1 * X
    
    return f_x
    
'''
1. ì„¤ëª… ì¤‘ 'ì†ì‹¤ í•¨ìˆ˜' íŒŒíŠ¸ì˜ ìˆ˜ì‹ì„ ì°¸ê³ í•´
   MSE ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”. 
'''
# f_x : ì˜ˆì¸¡ê°’, y : ì •ë‹µê°’

def Loss(f_x, y):
    # ì •ë‹µê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ì˜ ì œê³±ì„ í‰ê· 
    ls = np.mean(np.square(y - f_x))
    
    return ls

'''
2. ì„¤ëª… ì¤‘ 'Gradient' íŒŒíŠ¸ì˜ ë§ˆì§€ë§‰ ë‘ ìˆ˜ì‹ì„ ì°¸ê³ í•´ ë‘ ê°€ì¤‘ì¹˜
   w0ì™€ w1ì— ëŒ€í•œ gradientì¸ 'gradient0'ì™€ 'gradient1'ì„
   ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ gradient_descent í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.
   
   Step01. w0ì— ëŒ€í•œ gradientì¸ 'gradient0'ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
   
   Step02. w1ì— ëŒ€í•œ gradientì¸ 'gradient1'ì„ ì‘ì„±í•©ë‹ˆë‹¤.
'''

# ê°€ì¤‘ì¹˜ w, ì…ë ¥ X, ì •ë‹µ y

def gradient_descent(w0, w1, X, y):
    # ì•ì„œ ê³„ì‚°í•œ loss functionì„ ê° weightìœ¼ë¡œ í¸ë¯¸ë¶„ì„ í•˜ë©´ gradient ê°’ì´ ëœë‹¤.
    gradient0 = 2 * np.mean((y - (w0 + w1 * X)) * (-1))
    gradient1 = 2 * np.mean((y - (w0 + w1 * X)) * (-1 * X))
    
    return np.array([gradient0, gradient1])

'''
3. ì„¤ëª… ì¤‘ 'ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸' íŒŒíŠ¸ì˜ ë‘ ìˆ˜ì‹ì„ ì°¸ê³ í•´ 
   gradient descentë¥¼ í†µí•œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
   
   Step01. ì•ì„œ ì™„ì„±í•œ gradient_descent í•¨ìˆ˜ë¥¼ ì´ìš©í•´
           w0ì™€ w1ì— ëŒ€í•œ gradientì¸ 'gd'ë¥¼ ì •ì˜í•˜ì„¸ìš”.
           
   Step02. ë³€ìˆ˜ 'w0'ì™€ 'w1'ì— ë‘ ê°€ì¤‘ì¹˜ w0ì™€ w1ì„ 
           ì—…ë°ì´íŠ¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤. ì•ì„œ ì •ì˜í•œ
           ë³€ìˆ˜ 'gd'ì™€ ì´ë¯¸ ì •ì˜ëœ ë³€ìˆ˜ 'lr'ì„ ì‚¬ìš©í•˜ì„¸ìš”.
'''

def main():
    
    X = np.array([1,2,3,4]).reshape((-1,1))
    y = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))
    
    # íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
    w0 = 0
    w1 = 0
    
    # learning rate ì„¤ì •
    lr = 0.001
    
    # ë°˜ë³µ íšŸìˆ˜ 1000ìœ¼ë¡œ ì„¤ì •
    for i in range(1000):
    
        gd = gradient_descent(w0, w1, X, y)
        
        w0 = w0 - lr * gd[0]
        w1 = w1 - lr * gd[1]
        
        # 100íšŒë§ˆë‹¤ì˜ í•´ë‹¹ lossì™€ w0, w1 ì¶œë ¥
        if (i % 100 == 0):
        
            loss = Loss(linear_model(w0,w1,X),y)
        
            print("{}ë²ˆì§¸ loss : {}".format(i, loss))
            print("{}ë²ˆì§¸ w0, w1 : {}, {}".format(i, w0, w1),'\n')

    return w0, w1

if __name__ == '__main__':
    main()
```

### ì—­ì „íŒŒ(Back propagation) êµ¬í˜„í•˜ê¸°

ì—­ì „íŒŒ(Back propagation)ëŠ” ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  ëª¨ë¸ì„ ì´ë£¨ëŠ” ê°€ì¤‘ì¹˜ë“¤ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ê°œë°œëœ ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ ì¤‘ ê°€ì¥ ìœ ëª…í•˜ê³  ë„ë¦¬ ì“°ì´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ì—­ì „íŒŒë¥¼ ê°„ë‹¨í•˜ê²Œ ì‹¤ìŠµí•´ë³´ê¸° ìœ„í•´, í¼ì…‰íŠ¸ë¡  í•œ ê°œì˜ ê°€ì¤‘ì¹˜ë“¤ì„ ê°œì„ í•˜ëŠ” ì—­ì „íŒŒë¥¼ êµ¬í˜„í•´ ë³´ë„ë¡ í•©ë‹ˆë‹¤.

ì…ë ¥ì€ x1, x2, x3 ì„¸ ê°œì˜ ì •ìˆ˜ë¡œ ì£¼ì–´ì§€ê³ , ê°ê° w1, w2, w3ì˜ ê³„ìˆ˜ê°€ ê³±í•´ì ¸ **sigmoid í•¨ìˆ˜ë¥¼ í†µê³¼í•  ê°’**ì€ x1w1 + x2w2 + x3w3ê°€ ë©ë‹ˆë‹¤.

ì—¬ê¸°ì„œ w1, w2, w3ê°€ ë°”ë¡œ ìš°ë¦¬ê°€ ì´ë²ˆ ì‹¤ìŠµì—ì„œ ì•Œì•„ë‚´ì•¼ í•˜ëŠ” ê°€ì¤‘ì¹˜ì…ë‹ˆë‹¤.

x1w1 + x2w2 + x3w3ê°€ sigmoid í•¨ìˆ˜ë¥¼ ê±°ì¹˜ê³  ë‚˜ë©´ 0 ~ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì´ëŠ” íŠ¹ì • í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë  í™•ë¥ ì„ ë‚˜íƒ€ë‚´ë©°, 0.5ë³´ë‹¤ ì‘ì„ ê²½ìš° 0ìœ¼ë¡œ, 0.5ë³´ë‹¤ í´ ê²½ìš° 1ë¡œ ë¶„ë¥˜ëœë‹¤ê³  í•©ì‹œë‹¤.

ì´ì œ ì´ í¼ì…‰íŠ¸ë¡ ì„ í•™ìŠµì‹œí‚¤ë ¤ê³  í•©ë‹ˆë‹¤. ì¢€ ë” ì •í™•íˆ ì´ì•¼ê¸°í•˜ë©´, x1, x2, x3ì™€ ê·¸ í´ë˜ìŠ¤ yê°€ ì—¬ëŸ¬ ê°œ ì£¼ì–´ì§ˆ ë•Œ, yê°’ì„ ê°€ì¥ ì˜ ì˜ˆì¸¡í•˜ëŠ” w1, w2, w3ë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ìš°ë¦¬ê°€ ê°–ê³  ìˆëŠ” í›ˆë ¨ìš© ë°ì´í„°ê°€ ë‹¤ìŒê³¼ ê°™ì´ 3ê°œë¡œ ì£¼ì–´ì§„ë‹¤ê³  í•©ì‹œë‹¤.
```
(1, 0, 0) â€“> 0
(1, 0, 1) â€“> 1
(0, 0, 1) â€“> 1
```
ê·¸ë ‡ë‹¤ë©´ w1 = 0, w2 = 0, w3 = 1ì´ì–´ì•¼ í•¨ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë¬¼ë¡  ì´ì™€ ê°™ì€ ìµœì ì˜ w1, w2, w3ê°’ì„ ì²˜ìŒë¶€í„° ì•Œ ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ì„  ê°€ì¤‘ì¹˜ wë“¤ì„ ì´ˆê¸°í™”í•˜ê³ , ì´ë¥¼ ì—¬ëŸ¬ ë²ˆì˜ í•™ìŠµì„ ê±°ì³ ì•Œì•„ë‚´ì•¼ í•©ë‹ˆë‹¤.

ì¦‰, ì†ì‹¤ í•¨ìˆ˜(loss function)ì˜ gradient ê°’ì„ ì—­ì „íŒŒí•´ì„œ ë°›ì€ í›„, ê·¸ ê°’ì„ ì°¸ê³ í•˜ì—¬ ì†ì‹¤ í•¨ìˆ«ê°’ì„ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ w1, w2, w3ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

ì´ë•Œ, w1, w2, w3ì´ ì˜ ê°œì„ ë¼ì„œ ë” ì—…ë°ì´íŠ¸í•´ë„ ë³€í™”ê°€ ê±°ì˜ ì—†ì„ ë•Œê¹Œì§€ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

```
import math

def sigmoid(x) :
    return 1 / (1 + math.exp(-x))

'''
X, y ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” parameter (w1, w2, w3)ë¥¼ ë°˜í™˜í•˜ëŠ”
í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”. ì—¬ê¸°ì„œ XëŠ” (x1, x2, x3) ì˜ listì´ë©°, y ëŠ”
0 í˜¹ì€ 1ë¡œ ì´ë£¨ì–´ì§„ listì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, X, yëŠ” ë‹¤ìŒì˜ ê°’ì„
ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    X = [(1, 0, 0), (1, 0, 1), (0, 0, 1)]
    y = [0, 1, 1]
'''

'''
1. ì§€ì‹œ ì‚¬í•­ì„ ë”°ë¼ì„œ getParameters í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.

Step01. Xì˜ í•œ ì›ì†Œê°€ 3ê°œì´ë¯€ë¡œ ê°€ì¤‘ì¹˜ë„ 3ê°œê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        ì´ˆê¸° ê°€ì¤‘ì¹˜ wë¥¼ [1,1,1]ë¡œ ì •ì˜í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
        
        ë‹¨ìˆœíˆ f = 3, w = [1,1,1]ì´ë¼ê³  í•˜ëŠ” ê²ƒë³´ë‹¤ ì¢€ ë” 
        ì¢‹ì€ í‘œí˜„ì„ ìƒê°í•´ë³´ì„¸ìš”.
        
        
Step02. ì´ˆê¸° ê°€ì¤‘ì¹˜ wë¥¼ ëª¨ë¸ì— ë§ê²Œ ê³„ì† ì—…ë°ì´íŠ¸ í•´ì•¼í•©ë‹ˆë‹¤.
            
        ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ì´ˆê¸° ê°€ì¤‘ì¹˜ wì— ë”í•´ì§€ëŠ” ê°’ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        wPrimeì„ [0,0,0]ë¡œ ì •ì˜í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.  
            
        ë§ˆì°¬ê°€ì§€ë¡œ ë‹¨ìˆœíˆ wPrime = [0,0,0]ì´ë¼ê³  í•˜ëŠ” ê²ƒë³´ë‹¤
        ì¢€ ë” ì¢‹ì€ í‘œí˜„ì„ ìƒê°í•´ë³´ì„¸ìš”.
        
        
Step03. sigmoid í•¨ìˆ˜ë¥¼ í†µê³¼í•  rê°’ì„ ì •ì˜í•´ì•¼í•©ë‹ˆë‹¤. rì€ 
        Xì˜ ê° ê°’ê³¼ ê·¸ì— í•´ë‹¹í•˜ëŠ” ê°€ì¤‘ì¹˜ wì˜ ê³±ì˜ í•©ì…ë‹ˆë‹¤.
            
        ì¦‰, r = X_0_0 * w_0 + X_1_0 * w_0 + ... + X_2_2 * w_2
        ê°€ ë©ë‹ˆë‹¤.
            
        ê·¸ë¦¬ê³  sigmoid í•¨ìˆ˜ë¥¼ í†µê³¼í•œ rê°’ì„ vë¡œ ì •ì˜í•©ì‹œë‹¤.
    
    
Step04. ê°€ì¤‘ì¹˜ wê°€ ë”ì´ìƒ ì—…ë°ì´íŠ¸ê°€ ì•ˆë  ë•Œê¹Œì§€ ì—…ë°ì´íŠ¸ í•´ì¤˜ì•¼í•©ë‹ˆë‹¤.
        ì¦‰, ê°€ì¤‘ì¹˜ wì˜ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ë”í•´ì§€ëŠ” wPrimeì˜ ì ˆëŒ“ê°’ì´ ì–´ëŠ ì •ë„ê¹Œì§€
        ì‘ì•„ì§€ë©´ ì—…ë°ì´íŠ¸ë¥¼ ëë‚´ì•¼ í•©ë‹ˆë‹¤. 
            
        ê·¸ ê°’ì„ 0.001ë¡œ ì •í•˜ê³ , wPrimeì´ ê·¸ ê°’ì„ ë„˜ì§€ ëª»í•˜ë©´ ê°€ì¤‘ì¹˜ 
        ì—…ë°ì´íŠ¸ë¥¼ ëë‚´ë„ë¡ í•©ì‹œë‹¤. 
        
        ë‹¤ë§Œ wPrimeì˜ ì ˆëŒ“ê°’ì´ 0.001ë³´ë‹¤ ì‘ì•„ì§€ê¸° ì „ê¹Œì§€ëŠ” wì— wPrimeì„ ê³„ì†
        ë”í•˜ë©´ì„œ wë¥¼ ì—…ë°ì´íŠ¸ í•©ì‹œë‹¤.    
'''

def getParameters(X, y) :
    
    # Step01.
    # Xì˜ ì›ì†Œì˜ ê°œìˆ˜
    f = len(X[0])
    
    w = [1] * f
    
    values = []
    
    while True :
        
        # Step02.
        
        wPrime = [0] * f    
        
        vv = [] # sigmoidë¥¼ í†µê³¼í•œ rì´ ë“¤ì–´ê°ˆ ë¹ˆ ë¦¬ìŠ¤íŠ¸
        
        # Step03.
        
        for i in range(len(y)) :
            r = 0
            for j in range(f) :
                r = r + X[i][j] * w[j]
            
            v = sigmoid(r)
            vv.append(v)
            
            # wë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ wPrimeì„ ì—­ì „íŒŒë¥¼ ì´ìš©í•´ êµ¬í•˜ëŠ” ì‹
            for j in range(f) :
                wPrime[j] += -((v - y[i]) * v * (1-v) * X[i][j])
        
        # Step04.: ì—…ë°ì´íŠ¸ë¥¼ ì–¸ì œê¹Œì§€ í•  ê²ƒì¸ê°€
        
        flag = False
        
        for i in range(f) :
            if abs(wPrime[i]) >= 0.001 : # ì ˆëŒ€ê°’ wPrimeì˜ ì›ì†Œë“¤ì´ ëª¨ë‘ 0.001ë³´ë‹¤ í¬ë©´
                flag = True
                break
        
        if flag == False :
            break
        
        for j in range(f) :
            w[j] = w[j] + wPrime[j]
    
    return w

def main():
    
    '''
    ì´ ì½”ë“œëŠ” ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”.
    '''
    
    # X = [(1, 0, 0), (1, 0, 1), (0, 0, 1)]
    # y = [0, 1, 1]
    X = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)]
    y = [0, 0, 0, 1, 0, 1, 1, 1]
    
    '''
    # ì•„ë˜ì˜ ì˜ˆì œ ë˜í•œ í…ŒìŠ¤íŠ¸ í•´ë³´ì„¸ìš”.
    X = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)]
    y = [0, 0, 1, 1, 1, 1, 1, 1]

    # ì•„ë˜ì˜ ì˜ˆì œë¥¼ perceptronì´ 100% trainingí•  ìˆ˜ ìˆëŠ”ì§€ë„ í™•ì¸í•´ë´…ë‹ˆë‹¤.
    X = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)]
    y = [0, 0, 0, 1, 0, 1, 1, 1]
    '''
    
    print(getParameters(X, y))

if __name__ == "__main__":
    main()
```
