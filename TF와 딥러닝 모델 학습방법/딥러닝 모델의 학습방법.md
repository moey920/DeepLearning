# 딥러닝 모델의 학습 방법

- 딥러닝 모델이란 ? Hidden Layer가 3층 이상일 시 깊은 신경망이라는 의미의 Deep Learning 단어 사용
- 딥러닝 모델의 구성 요소   
    - 각 층을 구성하는 요소 : Nodes(Units) 
    - 노드간의 연결강도 : Weight(가중치)
    - 모델을 구성하는 층 : Layer
- 딥러닝에서의 학습이란 : Loss function을 최소화하기 위해 최적화 알고리즘을 적용

## 손실 함수(Loss Function)과 최적화(Optimization)
- 가장 적합한 가중치(w) 값을 찾는 것
- Loss Function : 예측값과 실제값간의 오차값
- Optimization : 오차값을 최소화하는 모델의 인자를 찾는 것
- 딥러닝 모델 학습 방법 : 예측값과 실제값 간의 오차값을 최소화하기 위해 오차값을 최소화하는 모델의 인자를 찾는 알고리즘을 적용

## 가장 기본적인 최적화 알고리즘, Gradient Descent(GD)

`𝑊𝑡+1 = 𝑊𝑡 − 𝛼∇𝐿𝑜𝑠𝑠(𝑊)`  
`Gradient(𝛁𝐋𝐨𝐬𝐬(𝐖))` = 특정 가중치에서의 기울기

> 신경망의 가중치들을 𝑊라고 했을 때, 손실함수 𝐿𝑜𝑠𝑠(𝑊)의 값을 최소화하기 위해 기울기 ∇𝐿𝑜𝑠𝑠(𝑊) 를 이용하는 방법

- 각 가중치들의 기울기를 구하는 방법 : 딥러닝에서는 역전파(Backpropagation)을 통해 각 가중치들의 기울기를 구할 수 있음
    - Boom times의 배경 : 역전파(Backpropogation) - 1980년대 AI 부흥을 이끔, 역전파를 이용해 뉴럴 네트워크를 학습할 수 있다는 것을 알게 됌

## 역전파(Backpropogation)의 정의

> 나의 목표 target 값과 실제 모델이 예측한 output 값이 얼마나 차이나는지 구한 후 오차값을 다시 뒤로 전파해가며 변수들을 갱신하는 알고리즘

- 순전파(Forward propagation) 정의 : 입력 값을 바탕으로 출력 값을 계산하는 과정
- 역전파(Backpropagation) : 이전 Layer의 변수들이 어떻게 되어야 정확한 target 값을 구해낼 수 있을까! Forward propagation의 반대 방향으로 이루어지는 과정

### Gradient descent 알고리즘 구현하기

##### Gradient descent

Gradient descent 알고리즘은 **손실 함수(loss function)의 미분값인 gradient를 이용**해 모델에게 맞는 **최적의 가중치(weight)**, 즉 손실 함수의 값을 최소화하는 가중치를 구할 수 있는 알고리즘입니다.

이번 실습에서는 Gradient descent 알고리즘을 직접 구현한 후, 이를 이용해 데이터를 가장 잘 설명하는 **선형 회귀 직선의 기울기와 y절편**, 즉 선형 회귀 모델에게 맞는 **최적의 가중치**를 찾아보겠습니다.

선형 회귀 직선의 수식은 다음과 같은 1차 함수 형태이며, 우리가 Gradient descent 알고리즘을 사용해 찾을 값, 즉 가중치는 w0, w1입니다.  
`f(x) = w0 + w1x`

##### 손실 함수 (loss function)
손실 함수(loss function)는 **실제값과 모델이 예측한 값 간의 차이를 계산해주는 함수**입니다. **손실 함수의 값은 가중치와 편향을 업데이트하는 데에 사용**됩니다. 여기서는 손실 함수로 MSE (Mean Squared Error)를 사용합니다.

**MSE는 평균 제곱 오차 함수**입니다.

##### 편미분

gradient에는 **편미분**이라는 개념이 들어갑니다. 따라서 gradient를 설명하기 전 편미분에 대해 간단하게 짚고 넘어가겠습니다. 

> 편미분이란 2개 이상의 변수를 가진 함수에서 우리가 미분할 하나의 변수를 제외한 나머지 변수들을 상수로 보고, 미분 대상인 그 변수로 미분하는 것입니다.

예를 들어 `f(x,y) = 2x^2+y`라는 수식이 있을 때, x에 대해서만 편미분한다면 `f‘(x,y) = 4x`가 되는 것입니다.

##### Gradient

gradient는 곧 **기울기 벡터**를 의미하며, **선형 함수의 각 파라미터의 편미분으로 구성된 열벡터로 정의**합니다.

학습률(learning rate)을 나타내는 α가 있고, gradient를 나타내는 수식인 ▽Loss(W)가 있습니다. 즉 이를 풀어서 쓰면 다음과 같은 열벡터 형태입니다.

`gradient = ▽Loss(W) = [∂Loss/∂w0, ∂Loss/∂w1]`

w0, w1에 대한 gradient를 구하기 위해 LossLoss를 각각에 대해 편미분하면 다음과 같습니다.  
`∂Loss/∂w0 = 2/N∑(yi-(w0+w1xi))(-1)`  
`∂Loss/∂w1 = 2/N∑(yi-(w0+w1xi))(-xi)`  

​	
 
##### 가중치 업데이트
위와 같이 구한 w0와 w1의 greadient와 학습률 α를 이용해 가중치를 업데이트하는 공식은 다음과 같습니다.

`w0^t+1 = w0^t - α∂Loss/∂w0`  
`w1^t+1 = w1^t - α∂Loss/∂w1`  

```
import numpy as np

# 사용할 1차 선형 회귀 모델, radient descent 알고리즘을 사용해 찾을 값, 즉 가중치는 w0, w1입니다.

def linear_model(w0, w1, X):
    
    f_x = w0 + w1 * X
    
    return f_x
    
'''
1. 설명 중 '손실 함수' 파트의 수식을 참고해
   MSE 손실 함수를 완성하세요. 
'''
# f_x : 예측값, y : 정답값

def Loss(f_x, y):
    # 정답값과 예측값의 차이의 제곱을 평균
    ls = np.mean(np.square(y - f_x))
    
    return ls

'''
2. 설명 중 'Gradient' 파트의 마지막 두 수식을 참고해 두 가중치
   w0와 w1에 대한 gradient인 'gradient0'와 'gradient1'을
   반환하는 함수 gradient_descent 함수를 완성하세요.
   
   Step01. w0에 대한 gradient인 'gradient0'를 작성합니다.
   
   Step02. w1에 대한 gradient인 'gradient1'을 작성합니다.
'''

# 가중치 w, 입력 X, 정답 y

def gradient_descent(w0, w1, X, y):
    # 앞서 계산한 loss function을 각 weight으로 편미분을 하면 gradient 값이 된다.
    gradient0 = 2 * np.mean((y - (w0 + w1 * X)) * (-1))
    gradient1 = 2 * np.mean((y - (w0 + w1 * X)) * (-1 * X))
    
    return np.array([gradient0, gradient1])

'''
3. 설명 중 '가중치 업데이트' 파트의 두 수식을 참고해 
   gradient descent를 통한 가중치 업데이트 코드를 작성하세요.
   
   Step01. 앞서 완성한 gradient_descent 함수를 이용해
           w0와 w1에 대한 gradient인 'gd'를 정의하세요.
           
   Step02. 변수 'w0'와 'w1'에 두 가중치 w0와 w1을 
           업데이트하는 코드를 작성합니다. 앞서 정의한
           변수 'gd'와 이미 정의된 변수 'lr'을 사용하세요.
'''

def main():
    
    X = np.array([1,2,3,4]).reshape((-1,1))
    y = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))
    
    # 파라미터 초기화
    w0 = 0
    w1 = 0
    
    # learning rate 설정
    lr = 0.001
    
    # 반복 횟수 1000으로 설정
    for i in range(1000):
    
        gd = gradient_descent(w0, w1, X, y)
        
        w0 = w0 - lr * gd[0]
        w1 = w1 - lr * gd[1]
        
        # 100회마다의 해당 loss와 w0, w1 출력
        if (i % 100 == 0):
        
            loss = Loss(linear_model(w0,w1,X),y)
        
            print("{}번째 loss : {}".format(i, loss))
            print("{}번째 w0, w1 : {}, {}".format(i, w0, w1),'\n')

    return w0, w1

if __name__ == '__main__':
    main()
```
