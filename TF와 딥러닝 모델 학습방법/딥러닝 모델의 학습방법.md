# 딥러닝 모델의 학습 방법

- 딥러닝 모델이란 ? Hidden Layer가 3층 이상일 시 깊은 신경망이라는 의미의 Deep Learning 단어 사용
- 딥러닝 모델의 구성 요소   
    - 각 층을 구성하는 요소 : Nodes(Units) 
    - 노드간의 연결강도 : Weight(가중치)
    - 모델을 구성하는 층 : Layer
- 딥러닝에서의 학습이란 : Loss function을 최소화하기 위해 최적화 알고리즘을 적용

## 손실 함수(Loss Function)과 최적화(Optimization)
- 가장 적합한 가중치(w) 값을 찾는 것
- Loss Function : 예측값과 실제값간의 오차값
- Optimization : 오차값을 최소화하는 모델의 인자를 찾는 것
- 딥러닝 모델 학습 방법 : 예측값과 실제값 간의 오차값을 최소화하기 위해 오차값을 최소화하는 모델의 인자를 찾는 알고리즘을 적용

## 가장 기본적인 최적화 알고리즘, Gradient Descent(GD)

`𝑊𝑡+1 = 𝑊𝑡 − 𝛼∇𝐿𝑜𝑠𝑠(𝑊)`  
`Gradient(𝛁𝐋𝐨𝐬𝐬(𝐖))` = 특정 가중치에서의 기울기

> 신경망의 가중치들을 𝑊라고 했을 때, 손실함수 𝐿𝑜𝑠𝑠(𝑊)의 값을 최소화하기 위해 기울기 ∇𝐿𝑜𝑠𝑠(𝑊) 를 이용하는 방법

- 각 가중치들의 기울기를 구하는 방법 : 딥러닝에서는 역전파(Backpropagation)을 통해 각 가중치들의 기울기를 구할 수 있음
    - Boom times의 배경 : 역전파(Backpropogation) - 1980년대 AI 부흥을 이끔, 역전파를 이용해 뉴럴 네트워크를 학습할 수 있다는 것을 알게 됌

## 역전파(Backpropogation)의 정의

> 나의 목표 target 값과 실제 모델이 예측한 output 값이 얼마나 차이나는지 구한 후 오차값을 다시 뒤로 전파해가며 변수들을 갱신하는 알고리즘

- 순전파(Forward propagation) 정의 : 입력 값을 바탕으로 출력 값을 계산하는 과정
- 역전파(Backpropagation) : 이전 Layer의 변수들이 어떻게 되어야 정확한 target 값을 구해낼 수 있을까! Forward propagation의 반대 방향으로 이루어지는 과정

### Gradient descent 알고리즘 구현하기

##### Gradient descent

Gradient descent 알고리즘은 **손실 함수(loss function)의 미분값인 gradient를 이용**해 모델에게 맞는 **최적의 가중치(weight)**, 즉 손실 함수의 값을 최소화하는 가중치를 구할 수 있는 알고리즘입니다.

이번 실습에서는 Gradient descent 알고리즘을 직접 구현한 후, 이를 이용해 데이터를 가장 잘 설명하는 **선형 회귀 직선의 기울기와 y절편**, 즉 선형 회귀 모델에게 맞는 **최적의 가중치**를 찾아보겠습니다.

선형 회귀 직선의 수식은 다음과 같은 1차 함수 형태이며, 우리가 Gradient descent 알고리즘을 사용해 찾을 값, 즉 가중치는 w0, w1입니다.  
`f(x) = w0 + w1x`

##### 손실 함수 (loss function)
손실 함수(loss function)는 **실제값과 모델이 예측한 값 간의 차이를 계산해주는 함수**입니다. **손실 함수의 값은 가중치와 편향을 업데이트하는 데에 사용**됩니다. 여기서는 손실 함수로 MSE (Mean Squared Error)를 사용합니다.

**MSE는 평균 제곱 오차 함수**입니다.

##### 편미분

gradient에는 **편미분**이라는 개념이 들어갑니다. 따라서 gradient를 설명하기 전 편미분에 대해 간단하게 짚고 넘어가겠습니다. 

> 편미분이란 2개 이상의 변수를 가진 함수에서 우리가 미분할 하나의 변수를 제외한 나머지 변수들을 상수로 보고, 미분 대상인 그 변수로 미분하는 것입니다.

예를 들어 `f(x,y) = 2x^2+y`라는 수식이 있을 때, x에 대해서만 편미분한다면 `f‘(x,y) = 4x`가 되는 것입니다.

##### Gradient

gradient는 곧 **기울기 벡터**를 의미하며, **선형 함수의 각 파라미터의 편미분으로 구성된 열벡터로 정의**합니다.

학습률(learning rate)을 나타내는 α가 있고, gradient를 나타내는 수식인 ▽Loss(W)가 있습니다. 즉 이를 풀어서 쓰면 다음과 같은 열벡터 형태입니다.

`gradient = ▽Loss(W) = [∂Loss/∂w0, ∂Loss/∂w1]`

w0, w1에 대한 gradient를 구하기 위해 LossLoss를 각각에 대해 편미분하면 다음과 같습니다.  
`∂Loss/∂w0 = 2/N∑(yi-(w0+w1xi))(-1)`  
`∂Loss/∂w1 = 2/N∑(yi-(w0+w1xi))(-xi)`  

​	
 
##### 가중치 업데이트
위와 같이 구한 w0와 w1의 greadient와 학습률 α를 이용해 가중치를 업데이트하는 공식은 다음과 같습니다.
ㅈ
​	
 

w_{1}^{t+1}=w_{1}^{t}-\alpha \frac{\partial Loss}{\partial w_1}
w 
1
t+1
​	
 =w 
1
t
​	
 −α 
∂w 
1
​	
 
∂Loss
​	
